---
perspective: platform
reviewer: platform-agent
reviewed_at: 2026-01-25T16:30:00Z
verdict: READY

findings:
  critical:
    - finding: |
        Aurora PostgreSQL schema design must support atomic transactions for Wishlist integration.
        SETS-008 requires creating Set before deleting Wishlist item to maintain data consistency.
        Impact: Data integrity critical; transaction failure could orphan items.
      stories: [SETS-007, SETS-008]
      mitigation: |
        Implement distributed transaction pattern with explicit rollback handlers.
        Add transaction logging to audit trail. Consider implementing saga pattern
        if cross-service transactions become complex.

    - finding: |
        Database indexing strategy not detailed. Performance risks at scale with large
        user collections (hundreds+ of sets per user).
      stories: [SETS-007]
      mitigation: |
        Create composite indexes on (userId, createdAt), (userId, isBuilt), (userId, setNumber).
        Monitor query performance in staging. Add pagination at API layer.

    - finding: |
        S3 image storage strategy depends on shared scraper service. No fallback strategy
        if image fetch fails during add flow.
      stories: [SETS-005]
      mitigation: |
        Allow manual image upload as fallback. Store image URLs separately from
        sourced images. Implement retry logic with exponential backoff.

  high:
    - finding: |
        Wishlist integration creates tight coupling between two epics. SETS-008 is high-risk
        with atomic transaction requirements and undo support.
      stories: [SETS-008, SETS-021]
      mitigation: |
        Design undo as separate API endpoint that restores state from audit trail.
        Pre-plan cross-epic testing strategy. Consider feature flags for rollout.

    - finding: |
        Shared scraper service reliability is blocking SETS-005, SETS-006, and SETS-018.
        No documented fallback if scraper fails or returns incomplete data.
      stories: [SETS-005, SETS-018]
      mitigation: |
        Establish SLA with scraper service owner. Implement circuit breaker pattern.
        Design graceful degradation: allow partial data entry with manual completion.

    - finding: |
        Optimistic updates in SETS-009, SETS-010 require careful state management.
        Network failures must cleanly revert UI without data loss.
      stories: [SETS-009, SETS-010]
      mitigation: |
        Implement optimistic update queue with local state backup. Store last-known
        good state. Design toast notifications with 3-5 second window for undo.

    - finding: |
        MOC Linking (SETS-011) creates many-to-many relationship requiring join table.
        Cross-epic dependency on MOC Instructions could delay delivery.
      stories: [SETS-011]
      mitigation: |
        Pre-create set_moc_links join table with indexes on both foreign keys.
        Coordinate timeline with MOC Instructions epic. Plan API contract early.

  medium:
    - finding: |
        Collection stats endpoint (SETS-016) performs aggregation across potentially
        hundreds of set records per user. No documented caching strategy.
      stories: [SETS-016]
      mitigation: |
        Implement materialized view or computed field in PostgreSQL. Cache results
        with 5-minute TTL. Use eventual consistency model for UI display.

    - finding: |
        Quantity stepper edge case (SETS-010) where decrementing below 1 should
        prompt delete creates complex state machine. Frontend must handle optimistically.
      stories: [SETS-010]
      mitigation: |
        Clearly define validation rules in API contract. Return explicit error codes
        for validation failures. Document frontend state flow.

    - finding: |
        Tag management integration (SETS-015) depends on shared tag system availability.
        No details on tag cardinality limits or performance characteristics.
      stories: [SETS-015]
      mitigation: |
        Establish shared tag system API contract. Discuss tag cardinality limits with
        platform team. Plan tag autocomplete performance.

    - finding: |
        Duplicate detection (SETS-018) performs set number lookup at add time.
        Query performance critical with many sets in collection.
      stories: [SETS-018]
      mitigation: |
        Add index on (userId, setNumber). Consider caching user's set numbers
        in memory during add flow. Implement fuzzy matching for variants.

  low:
    - finding: |
        Empty states (SETS-017) and mobile responsive (SETS-020) are frontend-only
        with minimal infrastructure impact. Low priority for platform review.
      stories: [SETS-017, SETS-020]
      mitigation: |
        Standard implementation patterns. No special platform considerations needed.

---

infrastructure_needs:
  - component: "Aurora PostgreSQL - sets table"
    requirement: |
      Primary table for set collection items. Must support:
      - UUID primary key (id)
      - Foreign key to users (userId)
      - Nullable columns for optional metadata (imageUrl, sourceUrl, theme, etc.)
      - Numeric fields for prices/counts with decimal precision
      - Composite indexes on (userId, createdAt), (userId, isBuilt), (userId, setNumber)
    stories: [SETS-007]
    complexity: high

  - component: "Aurora PostgreSQL - set_moc_links join table"
    requirement: |
      Many-to-many relationship between sets and MOCs.
      - Composite primary key: (setId, mocId)
      - Foreign keys to sets and mocs tables
      - Indexes on both foreign keys for query performance
      - Timestamps for audit trail
    stories: [SETS-011]
    complexity: medium

  - component: "Aurora PostgreSQL - query performance"
    requirement: |
      Gallery queries must return <100ms for typical user (50-200 sets).
      Filtering by build status, sorting by purchase date common operations.
      Pagination support for collections >500 items.
    stories: [SETS-007, SETS-014]
    complexity: medium

  - component: "S3 storage - set images"
    requirement: |
      Store product images from scraper or user uploads.
      Estimate: ~2-3KB per image, 1000+ sets per active user.
      Organize by userId for access control.
      CloudFront distribution for fast retrieval.
    stories: [SETS-005, SETS-006]
    complexity: low

  - component: "Lambda - Sets CRUD endpoints"
    requirement: |
      5 main endpoints: POST, GET list, GET detail, PATCH, DELETE.
      Each with proper authentication (userId from auth context),
      input validation, error handling, transaction support for Wishlist integration.
      Estimated cold start impact: <500ms acceptable.
    stories: [SETS-007]
    complexity: high

  - component: "Lambda - Wishlist integration endpoint"
    requirement: |
      POST /api/sets/from-wishlist with atomic transaction semantics.
      Must create Set first, then delete Wishlist item.
      Implement distributed transaction with rollback on failure.
      Return transaction ID for undo support.
    stories: [SETS-008]
    complexity: high

  - component: "Lambda - Update endpoints (build status, quantity)"
    requirement: |
      PATCH /api/sets/:id/build-status and PATCH /api/sets/:id/quantity
      Support optimistic updates with clean error handling.
      Return transaction ID for undo capability.
      Fast response required for snappy UI (target <200ms).
    stories: [SETS-009, SETS-010]
    complexity: medium

  - component: "Lambda - Duplicate detection endpoint"
    requirement: |
      GET /api/sets/check-duplicate with setNumber parameter.
      Return count of existing sets with same number.
      Sub-100ms response critical for add flow UX.
      May use caching if large user collections slow query.
    stories: [SETS-018]
    complexity: low

  - component: "Lambda - Stats aggregation endpoint"
    requirement: |
      GET /api/sets/stats computes total sets, unique sets, total pieces, total value.
      Consider materialized view or cache to avoid aggregating 100+ records per query.
      Target <500ms response even for large collections.
    stories: [SETS-016]
    complexity: medium

  - component: "API Gateway - Sets resource routing"
    requirement: |
      Route /api/sets/* to Lambda functions with proper HTTP method dispatch.
      Implement request/response validation using OpenAPI schemas.
      Add CORS headers for web client. Request ID logging for tracing.
    stories: [SETS-007, SETS-008, SETS-009, SETS-010]
    complexity: low

  - component: "Shared scraper service integration"
    requirement: |
      Coordinate with Wishlist epic's scraper service.
      Define timeout (suggest 5s), retry policy, error codes.
      Handle partial failures (metadata without image, etc.).
      Document supported retailers and LEGO.com variants.
    stories: [SETS-005]
    complexity: medium

  - component: "Audit logging - transaction history"
    requirement: |
      Log all set creation, updates, deletions with userId, timestamp, transaction ID.
      Required for undo feature (SETS-008, SETS-009, SETS-010).
      Separate audit table or CloudWatch logs for compliance.
    stories: [SETS-008, SETS-009, SETS-010]
    complexity: medium

---

database_concerns:
  - concern: "Atomic transaction failure in Wishlist integration"
    table: "sets, wishlist_items"
    mitigation: |
      Implement two-phase commit or saga pattern. Create Set with unique transaction ID
      first, then delete Wishlist. If delete fails, Set remains orphaned but marked with
      failed transaction ID. Cleanup job can resolve later. Never delete Wishlist
      without confirming Set creation.

  - concern: "Large collection pagination"
    table: "sets"
    mitigation: |
      Implement offset-limit pagination at API layer with max 100 items/page.
      Add cursor-based pagination option for large collections (>10K items).
      Denormalize or cache frequently accessed aggregations (built count, total pieces).

  - concern: "Index cardinality on setNumber lookups"
    table: "sets"
    mitigation: |
      Composite index on (userId, setNumber) critical for duplicate detection.
      Consider partial index on (userId, setNumber) WHERE deletedAt IS NULL
      if soft-deletes implemented later. Monitor index fragmentation.

  - concern: "Image URL storage strategy"
    table: "sets"
    mitigation: |
      Store S3 object key separately from sourceUrl. If S3 image deleted later,
      can still access sourceUrl. Consider archival strategy: move old images to
      Glacier after 1 year if space becomes concern.

  - concern: "Many-to-many join table bloat"
    table: "set_moc_links"
    mitigation: |
      Monitor cardinality: one set linked to many MOCs creates many rows.
      For large user collections, consider denormalization or caching
      frequently accessed MOC links. Index (setId) for finding MOCs by set.

  - concern: "Build status toggle hot-path performance"
    table: "sets"
    mitigation: |
      Single-column update (isBuilt) should be very fast. Ensure no blocking
      locks from other queries. Consider separate volatile column for real-time
      features if build status queried frequently in stats.

---

performance_risks:
  - risk: "Gallery list query N+1 problem if MOC links loaded per item"
    trigger: |
      User opens gallery with 50+ sets. If API loads MOC links for each set,
      N+1 queries result. Users see slow page load, potential timeout.
    mitigation: |
      Use SQL JOIN to fetch sets and MOC counts in single query.
      Return aggregate count (e.g., "{mocCount: 3}") instead of full MOC list in gallery.
      Full MOC list only on detail view with separate API call.

  - risk: "Scraper timeout if LEGO.com slow or unreachable"
    trigger: |
      User clicks 'Fetch Product Info' button. If scraper service takes >10s
      or fails silently, user sees hanging modal with no error feedback.
    mitigation: |
      Implement 5-second timeout with clear error toast: "Couldn't fetch product info.
      Enter details manually?" Allow manual entry as fallback. Retry button for resilience.

  - risk: "Optimistic update rollback causing UI flicker"
    trigger: |
      User toggles build status. UI changes immediately. Network fails. UI reverts.
      Repeated revert attempts confuse user. Undo toast covers revert feedback.
    mitigation: |
      Show explicit error toast on failure: "Couldn't save. Retrying..." with Retry button.
      Queue updates: don't allow new updates while retry in flight. Disable toggle during retry.

  - risk: "Duplicate detection query slow for large collections"
    trigger: |
      User adds set. Duplicate check queries 500+ sets by setNumber.
      Unindexed query takes 2+ seconds. User sees 2s delay before modal.
    mitigation: |
      Ensure (userId, setNumber) index exists. Consider client-side cache of user's
      set numbers during session. Pre-load set list in background after gallery loads.

  - risk: "Stats aggregation timeout for power users with 1000+ sets"
    trigger: |
      User opens dashboard. Stats endpoint aggregates 1000 sets.
      Full table scan takes 5+ seconds. Endpoint times out (Lambda 30s limit).
    mitigation: |
      Implement materialized view materialized_set_stats refreshed hourly.
      Or store aggregates in separate table updated on each set change.
      Cache with 5-minute TTL in memory or ElastiCache. Accept eventual consistency.

  - risk: "Image upload/storage failure during add flow"
    trigger: |
      User adds set via URL. Scraper returns image. S3 upload fails.
      Add flow blocked. No fallback to proceed without image.
    mitigation: |
      Make image optional. Allow add without image or with placeholder.
      Queue failed image uploads for async retry. Notify user: "Image saved separately."
      Implement async image optimization pipeline.

  - risk: "MOC linking query cross-epic dependency delays feature"
    trigger: |
      SETS-011 depends on MOC Instructions epic. If MOC API not ready,
      SETS-011 blocked. Parallel work on other features delayed.
    mitigation: |
      Create mock MOC API for integration testing. Define API contract early.
      Implement feature flag to enable/disable MOC linking. Start development
      against mock, then switch to real API.

---

scalability_assessment:
  concerns:
    - concern: "Gallery query scale"
      current: "Assume 50-200 sets per user typical, 10K+ for power users"
      issue: |
        Unindexed or poorly indexed queries could slow at scale.
        Gallery list should return in <100ms for typical user.
        Filter/sort operations must be optimized.
      recommendation: |
        Implement query plan analysis in staging. Use EXPLAIN ANALYZE
        for all gallery queries. Monitor latency percentiles (p95, p99).
        Add database read replicas if single-primary becomes bottleneck.

    - concern: "S3 storage and CloudFront distribution"
      current: "2-3KB per image, potentially 1000+ images per active user"
      issue: |
        S3 costs scale linearly with storage. CloudFront distribution
        needed for fast image retrieval. Stale image cleanup strategy needed.
      recommendation: |
        Implement image expiration policy: move to Glacier after 1 year.
        Monitor cost per user. Consider image compression optimization.
        Use CloudFront with aggressive caching (Cache-Control: max-age=1y).

    - concern: "Lambda cold start impact on add flow"
      current: "5+ Lambda functions for Sets CRUD"
      issue: |
        Cold start could add 500ms+ latency during add flow.
        User perceives slow "Add" button response.
      recommendation: |
        Use Lambda Provisioned Concurrency for frequently-called endpoints
        (GET list, POST create). Monitor cold start rate. Accept ~100ms latency
        for infrequent operations (DELETE, PATCH detail fields).

    - concern: "Database connection pooling"
      current: "Aurora PostgreSQL serverless or provisioned unknown"
      issue: |
        If using RDS (non-serverless), each Lambda invocation opens new connection.
        Connection pool exhaustion under high concurrency.
      recommendation: |
        Use Aurora Serverless or implement RDS Proxy for connection pooling.
        Monitor active connections. Set max connections per Lambda to prevent
        pool exhaustion during traffic spikes.

    - concern: "Wishlist integration atomicity at scale"
      current: "Distributed transaction between two services"
      issue: |
        If Wishlist service slow or unreliable, Sets service can't guarantee
        consistency. Eventual consistency window could be minutes.
      recommendation: |
        Implement compensating transaction (saga pattern) for rollback.
        Add retry queue for failed transactions. Monitor cross-service latency.
        Consider circuit breaker to fail fast if Wishlist service degraded.

  recommendations:
    - recommendation: |
        Establish clear SLAs for dependent services:
        - Scraper service: <2s latency for product fetch, 99.5% availability
        - Wishlist service: <100ms latency for transaction operations
        - MOC service: <100ms latency for MOC picker queries
        Create service monitoring dashboard tracking these metrics.

    - recommendation: |
        Implement comprehensive database monitoring:
        - Query latency percentiles (p50, p95, p99)
        - Index usage and fragmentation
        - Connection pool utilization
        - Replication lag (if read replicas added)
        Set up CloudWatch alarms for anomalies.

    - recommendation: |
        Design for graceful degradation:
        - Allow add without scraper data (manual entry fallback)
        - Allow add without image (placeholder or user upload)
        - Allow add without MOC linking (feature flag off)
        - Queue operations if dependent services slow (eventual consistency)

    - recommendation: |
        Plan for future growth:
        - Collections with 10K+ sets: pagination mandatory
        - Power users accessing frequently: consider caching layer (ElastiCache)
        - Multi-region support: consider DynamoDB for global data if needed
        - Real-time updates: WebSocket support for collaborative features

    - recommendation: |
        Implement comprehensive audit logging:
        - All set mutations (create, update, delete, link/unlink MOCs)
        - Cross-epic transactions (Wishlist integration)
        - Performance metrics (scraper latency, API response times)
        - Error tracking (scraper failures, validation failures)
        Use for debugging and compliance.

---

deployment_considerations:
  - consideration: "Database schema migrations must be coordinated"
    impact: |
      Sets table creation (SETS-007) is critical path. set_moc_links table (SETS-011)
      requires MOC Instructions schema to exist or be created simultaneously.
      Rollback strategy needed if migration fails mid-deployment.
    plan: |
      Create migration scripts with checksums for idempotency.
      Test migrations in staging environment with production data snapshot.
      Plan rollback plan (reverse migration scripts). Communicate timeline
      to MOC Instructions epic owner.

  - consideration: "Lambda function versioning and canary deployments"
    impact: |
      Wishlist integration (SETS-008) is high-risk. Any bug could corrupt data.
      Recommend gradual rollout with canary deployment.
    plan: |
      Deploy SETS-007 (base API) with 100% traffic first. Monitor for 24 hours.
      Deploy SETS-008 with 10% traffic initially. Gradually increase to 100%.
      Monitor error rates, latency, database consistency. Have quick rollback plan.

  - consideration: "Feature flags for dependent services"
    impact: |
      MOC linking (SETS-011) and tag management (SETS-015) depend on other epics.
      If those services unavailable at deploy time, features should degrade gracefully.
    plan: |
      Implement feature flag for each dependent feature.
      Default to OFF during initial deployment.
      Enable only after dependent services confirmed ready in production.
      Use AWS AppConfig or similar for dynamic feature flag management.

  - consideration: "API contract documentation and versioning"
    impact: |
      Multiple frontend consumers (Sets gallery, dashboard) and other services
      (Wishlist, MOC) depend on Sets API. Breaking changes could cascade failures.
    plan: |
      Document API contract in OpenAPI 3.0 format.
      Use API versioning (v1, v2) for backward compatibility.
      Deprecate old endpoints with 6-month notice.
      Test API contracts in integration tests against OpenAPI schema.

  - consideration: "S3 image storage cleanup and archival"
    impact: |
      Over time, deleted sets' images accumulate in S3.
      Storage costs increase. Need cleanup strategy.
    plan: |
      Implement S3 lifecycle policy: move images to Glacier after 90 days
      if associated set deleted. Permanently delete after 1 year.
      Monitor S3 costs in AWS Cost Explorer.
      Set up S3 Intelligent-Tiering for automatic cost optimization.

  - consideration: "Database backup and disaster recovery"
    impact: |
      Sets table is critical for user data. Aurora automatic backups must be
      configured. Recovery RTO/RPO must be defined.
    plan: |
      Enable Aurora automated backups with 30-day retention.
      Test backup restoration quarterly. Document RTO (recovery time objective)
      and RPO (recovery point objective) for stakeholders.
      For critical production, consider cross-region backup.

  - consideration: "Cross-epic deployment coordination"
    impact: |
      SETS-008 (Wishlist integration), SETS-011 (MOC linking), SETS-015 (tag management)
      all depend on other epics being deployed first.
    plan: |
      Create deployment checklist for each story.
      Coordinate with Wishlist, MOC, and tag management epic owners.
      Document dependencies in deployment runbook.
      Use feature flags to enable gradual rollout.

  - consideration: "Monitoring and alerting setup"
    impact: |
      Need proactive monitoring to catch issues before users report.
      Critical paths: gallery load, add flow, Wishlist integration.
    plan: |
      Set up CloudWatch dashboards for:
        - API latency (p95, p99)
        - Error rates by endpoint
        - Database query performance
        - S3 and scraper service health
      Create SNS alerts for:
        - Error rate >1% on any endpoint
        - Latency p95 >500ms
        - Database connection pool utilization >80%
        - Scraper service failures
      Daily review of metrics for first 2 weeks post-deployment.

---
