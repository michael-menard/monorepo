# Operational Runbooks
# Generated: 2026-01-24
# Updated: 2026-01-25 - Added Knowledge Base MCP runbooks
#
# Procedures for deployment, incidents, and maintenance

---

# =============================================================================
# KNOWLEDGE BASE MCP SERVER
# =============================================================================

- id: runbook-kb-001
  content: |
    **Connect Knowledge Base to Claude Code**

    The KB runs as an MCP server that Claude connects to via stdio.

    **Prerequisites:**
    - Docker Desktop running
    - OpenAI API key
    - Package built (`pnpm build --filter=@repo/knowledge-base`)

    **Step 1: Start PostgreSQL**
    ```bash
    cd apps/api/knowledge-base
    docker-compose up -d
    ```

    **Step 2: Configure environment**
    ```bash
    cp .env.example .env
    # Edit .env:
    #   KB_DB_PASSWORD=YourSecurePassword123!
    #   OPENAI_API_KEY=sk-...
    ```

    **Step 3: Initialize database**
    ```bash
    pnpm db:push     # Push schema to DB
    pnpm validate:env  # Verify config
    ```

    **Step 4: Register with Claude**
    Create or edit `~/.claude/mcp.json`:
    ```json
    {
      "mcpServers": {
        "knowledge-base": {
          "command": "node",
          "args": [
            "/Users/YOUR_USER/Development/Monorepo/apps/api/knowledge-base/dist/mcp-server/index.js"
          ],
          "env": {
            "DATABASE_URL": "postgresql://kbuser:YourPassword@localhost:5433/knowledgebase",
            "OPENAI_API_KEY": "sk-..."
          }
        }
      }
    }
    ```

    **Step 5: Restart Claude Code**
    Quit and reopen Claude Code. The KB tools will appear in tool list.

    **Available MCP Tools:**
    - `kb_add` - Add new knowledge entry
    - `kb_get` - Get entry by ID
    - `kb_update` - Update existing entry
    - `kb_delete` - Delete entry
    - `kb_list` - List entries with filters
    - `kb_search` - Semantic + keyword search
    - `kb_get_related` - Find similar entries
    - `kb_stats` - Database statistics
    - `kb_health` - Server health check
  entry_type: runbook
  roles: [dev]
  tags: [knowledge-base, mcp, claude, setup, connection]
  source_file: apps/api/knowledge-base/README.md
  confidence: 1.0

- id: runbook-kb-002
  content: |
    **How Embeddings/Vectorization Work**

    The KB uses OpenAI's text-embedding-3-small model to convert text into 1536-dimensional vectors for semantic search.

    **Flow: Text → Vector**
    ```
    1. You call kb_add({ content: "Your text here", ... })
    2. Content is hashed (SHA-256) for deduplication
    3. Hash checked against embedding_cache table
    4. If cache miss: OpenAI API called → 1536-dim vector returned
    5. Vector stored in embedding_cache (keyed by hash)
    6. Entry stored in knowledge_entries with vector
    ```

    **Cost:**
    - Model: text-embedding-3-small
    - Price: $0.00002 per 1K tokens (~750 words)
    - Caching: Same content never re-embedded (hash match)

    **Search Flow:**
    ```
    1. You call kb_search({ query: "how to fix X" })
    2. Query text embedded via same process
    3. pgvector finds nearest vectors (cosine similarity)
    4. Keyword search also runs (PostgreSQL FTS)
    5. Results merged via RRF (Reciprocal Rank Fusion)
    6. Top results returned with similarity scores
    ```

    **Key Tables:**
    - `knowledge_entries` - Content + embedding vector
    - `embedding_cache` - Hash → vector cache (avoids re-embedding)

    **Index:**
    - IVFFlat index on embedding column
    - Approximate nearest neighbor (ANN) for speed
    - `lists=100` parameter (tune for dataset size)
  entry_type: runbook
  roles: [dev]
  tags: [knowledge-base, embeddings, vectors, openai, search]
  source_file: apps/api/knowledge-base/src/embedding-client/index.ts
  confidence: 1.0

- id: runbook-kb-003
  content: |
    **Content Preparation (No Auto-Chunking)**

    ⚠️ **IMPORTANT:** The KB does NOT automatically chunk long documents.

    You must prepare content appropriately before adding:

    **Guidelines:**
    - Each entry should be a discrete, self-contained piece of knowledge
    - Ideal size: 100-500 words (fits well in context windows)
    - Maximum: ~8000 tokens (OpenAI embedding limit)
    - Too short (<20 words): May not embed meaningfully
    - Too long (>2000 words): Split into logical sections

    **Good Entry Examples:**
    ```yaml
    # Single concept
    - content: "When using Drizzle ORM, always call db.push() in dev
                and db.migrate() in production. Push is faster but
                doesn't create migration files..."
      tags: [drizzle, database, migrations]

    # One procedure
    - content: "To debug Lambda locally: 1) Set AUTH_BYPASS=true
                2) Run pnpm sls:offline 3) Test at localhost:4000..."
      tags: [lambda, debug, local]
    ```

    **Bad Entry Examples:**
    ```yaml
    # Too vague
    - content: "Databases are important"
      # Won't match specific queries

    # Too long (entire README)
    - content: "[5000 words of documentation]"
      # Split into sections instead

    # Multiple unrelated topics
    - content: "Use React 19. Also, Lambda needs 256MB. And
                Tailwind uses utility classes..."
      # Split into separate entries
    ```

    **Chunking Strategy for Long Docs:**
    1. Split by logical section (## headers in markdown)
    2. Each chunk gets its own entry
    3. Use consistent tags to group related chunks
    4. Include context in each chunk ("In the Auth module...")
  entry_type: runbook
  roles: [dev, pm]
  tags: [knowledge-base, content, chunking, preparation]
  source_file: apps/api/knowledge-base
  confidence: 1.0

- id: runbook-kb-004
  content: |
    **Quick Reference: KB Commands**

    **Docker:**
    ```bash
    cd apps/api/knowledge-base

    # Start database
    docker-compose up -d

    # Stop database
    docker-compose down

    # Reset database (delete all data)
    docker-compose down -v && docker-compose up -d && pnpm db:push

    # View logs
    docker-compose logs -f kb-postgres
    ```

    **Database:**
    ```bash
    pnpm db:push      # Push schema (dev)
    pnpm db:migrate   # Run migrations (prod)
    pnpm db:studio    # Open Drizzle Studio UI
    pnpm db:seed      # Seed sample data
    pnpm validate:env # Check env vars
    ```

    **Build & Test:**
    ```bash
    pnpm build --filter=@repo/knowledge-base
    pnpm test --filter=@repo/knowledge-base
    pnpm mcp:start    # Start MCP server manually
    ```

    **Direct DB Access:**
    ```bash
    # Connect to PostgreSQL
    docker exec -it knowledge-base-postgres psql -U kbuser -d knowledgebase

    # Useful queries
    SELECT count(*) FROM knowledge_entries;
    SELECT id, left(content, 50), tags FROM knowledge_entries LIMIT 10;
    SELECT count(*) FROM embedding_cache;
    ```
  entry_type: runbook
  roles: [dev]
  tags: [knowledge-base, commands, docker, database]
  source_file: apps/api/knowledge-base/package.json
  confidence: 1.0

- id: runbook-kb-005
  content: |
    **Troubleshooting: KB Connection Issues**

    **"Cannot connect to MCP server"**
    1. Check Docker is running: `docker ps | grep knowledge-base`
    2. Check package is built: `ls apps/api/knowledge-base/dist/mcp-server/index.js`
    3. Verify mcp.json path is absolute (not relative)
    4. Check env vars in mcp.json match your .env

    **"OPENAI_API_KEY is required"**
    - Add OPENAI_API_KEY to the `env` block in mcp.json
    - OR set it in your shell profile (~/.zshrc)

    **"Connection refused to PostgreSQL"**
    1. Start Docker: `docker-compose up -d`
    2. Wait for healthy: `docker-compose ps` (should show "healthy")
    3. Check port: default is 5433, not 5432

    **"Extension vector not found"**
    - Wrong Docker image. Must use `pgvector/pgvector:pg16`
    - Reset: `docker-compose down -v && docker-compose up -d`

    **Tools not appearing in Claude**
    1. Restart Claude Code completely (Cmd+Q, reopen)
    2. Check mcp.json syntax: `cat ~/.claude/mcp.json | jq .`
    3. Check Claude logs for MCP errors

    **Embeddings failing**
    - Check OPENAI_API_KEY is valid
    - Check rate limits: reduce OPENAI_MAX_CONCURRENT_REQUESTS
    - Check timeout: increase OPENAI_TIMEOUT_MS
  entry_type: runbook
  roles: [dev]
  tags: [knowledge-base, troubleshooting, mcp, connection]
  source_file: apps/api/knowledge-base/README.md
  confidence: 1.0

- id: runbook-kb-006
  content: |
    **Using the KB Tools in Claude**

    Once connected, Claude has access to these tools:

    **Add Knowledge:**
    ```
    "Add this to the knowledge base: When deploying Lambda,
    always set memory to at least 256MB for Node.js functions.
    Tag it with 'lambda' and 'deployment'."
    ```
    → Claude calls `kb_add({ content: "...", tags: ["lambda", "deployment"], role: "dev" })`

    **Search Knowledge:**
    ```
    "Search the knowledge base for how to debug Lambda locally"
    ```
    → Claude calls `kb_search({ query: "debug Lambda locally", limit: 5 })`

    **Find Related:**
    ```
    "What other knowledge do we have related to entry abc-123?"
    ```
    → Claude calls `kb_get_related({ id: "abc-123", limit: 5 })`

    **List with Filters:**
    ```
    "Show me all knowledge entries tagged with 'security'"
    ```
    → Claude calls `kb_list({ tags: ["security"], limit: 20 })`

    **Check Health:**
    ```
    "Is the knowledge base healthy?"
    ```
    → Claude calls `kb_health({})` → Returns DB status, OpenAI status, uptime

    **Roles:**
    - `pm` - Product management knowledge
    - `dev` - Development/engineering knowledge
    - `qa` - Testing/QA knowledge
    - `all` - Applies to everyone
  entry_type: runbook
  roles: [dev, pm, qa]
  tags: [knowledge-base, mcp, tools, usage, claude]
  source_file: apps/api/knowledge-base/src/mcp-server/tool-schemas.ts
  confidence: 1.0

# =============================================================================
# LOCAL DEVELOPMENT
# =============================================================================

- id: runbook-dev-001
  content: |
    **Start Local Development Environment**

    ```bash
    # 1. Start database (Docker)
    docker-compose up -d postgres

    # 2. Run migrations
    pnpm db:migrate

    # 3. Seed data (optional)
    pnpm seed

    # 4. Start all apps
    pnpm dev
    ```

    URLs:
    - Frontend: http://localhost:5173
    - API (Vercel): http://localhost:3000
    - Storybook: http://localhost:6006

    Note: `pnpm seed` may fail on seedSets() - use `pnpm seed:gallery` as workaround.
  entry_type: runbook
  roles: [dev]
  tags: [local, development, setup, docker]
  source_file: README.md
  confidence: 1.0

- id: runbook-dev-002
  content: |
    **Run Scoped Verification**

    Full monorepo verification may fail due to pre-existing issues.
    Use scoped commands for new/changed files:

    ```bash
    # Lint specific file
    pnpm eslint path/to/file.ts

    # Type-check specific package
    pnpm --filter @repo/your-package check-types

    # Test specific package
    pnpm --filter @repo/your-package test

    # Test with coverage
    pnpm --filter @repo/your-package test -- --coverage
    ```

    Pre-existing failures: @repo/app-dashboard, @repo/gallery-core, others.
  entry_type: runbook
  roles: [dev]
  tags: [verification, lint, test, scoped]
  source_file: plans/stories/LESSONS-LEARNED.md
  confidence: 1.0

- id: runbook-dev-003
  content: |
    **Debug Lambda Locally**

    ```bash
    # Start serverless offline
    cd apps/api
    pnpm sls:offline

    # Lambda runs at http://localhost:4000
    # Invoke specific function
    curl http://localhost:4000/dev/api/mocs
    ```

    For Vercel handlers:
    ```bash
    cd apps/api/platforms/vercel
    vercel dev
    # Runs at http://localhost:3000
    ```

    Set `AUTH_BYPASS=true` in .env to skip JWT validation locally.
  entry_type: runbook
  roles: [dev]
  tags: [debug, lambda, serverless, local]
  source_file: apps/api/serverless.yml
  confidence: 1.0

# =============================================================================
# DEPLOYMENT
# =============================================================================

- id: runbook-deploy-001
  content: |
    **Deploy to Staging**

    ```bash
    # 1. Ensure on main branch with latest
    git checkout main && git pull

    # 2. Run full verification
    pnpm lint:all && pnpm check-types:all && pnpm test:all

    # 3. Deploy API to staging
    cd apps/api
    pnpm sls:deploy --stage staging

    # 4. Deploy frontend to staging
    cd apps/web/main-app
    pnpm build
    # Upload to S3/CloudFront or Vercel
    ```

    Staging uses reduced resources (Aurora 0.5-2 ACU, 128-256MB Lambda).
  entry_type: runbook
  roles: [dev]
  tags: [deploy, staging, serverless]
  source_file: apps/api/serverless.yml
  confidence: 1.0

- id: runbook-deploy-002
  content: |
    **Deploy to Production**

    ```bash
    # 1. Create release tag
    git tag -a v1.x.x -m "Release v1.x.x"
    git push origin v1.x.x

    # 2. Deploy API
    cd apps/api
    pnpm sls:deploy --stage production

    # 3. Deploy frontend
    cd apps/web/main-app
    pnpm build
    # Upload to production S3/CloudFront

    # 4. Verify health
    curl https://api.yourdomain.com/health
    ```

    Production config: Aurora 0.5-4 ACU, optimized Lambda memory, 5 version retention.
  entry_type: runbook
  roles: [dev]
  tags: [deploy, production, release]
  source_file: apps/api/serverless.yml
  confidence: 1.0

- id: runbook-deploy-003
  content: |
    **Rollback Deployment**

    ```bash
    # Lambda rollback (uses retained versions)
    cd apps/api
    pnpm sls rollback --stage production --timestamp <timestamp>

    # Or deploy previous tag
    git checkout v1.x.x
    pnpm sls:deploy --stage production

    # Frontend rollback
    # Restore previous S3 version or redeploy from tag
    ```

    Production retains 5 Lambda versions. Staging retains 3.
  entry_type: runbook
  roles: [dev]
  tags: [rollback, deployment, recovery]
  source_file: apps/api/serverless.yml
  confidence: 1.0

# =============================================================================
# DATABASE
# =============================================================================

- id: runbook-db-001
  content: |
    **Run Database Migrations**

    ```bash
    # Generate migration from schema changes
    pnpm --filter @repo/database-schema db:generate

    # Apply migrations
    pnpm --filter @repo/database-schema db:migrate

    # Push schema directly (dev only, no migration file)
    pnpm --filter @repo/database-schema db:push
    ```

    Drizzle Kit handles migration files in `drizzle/` directory.
    Always generate + review migration before applying to staging/prod.
  entry_type: runbook
  roles: [dev]
  tags: [database, migrations, drizzle]
  source_file: packages/backend/database-schema
  confidence: 1.0

- id: runbook-db-002
  content: |
    **Connect to Production Database**

    ```bash
    # Via AWS CLI (assumes IAM auth configured)
    aws rds generate-db-auth-token \
      --hostname <aurora-endpoint> \
      --port 5432 \
      --username <db-user> \
      --region us-east-1

    # Connect with psql
    PGPASSWORD=<auth-token> psql \
      -h <aurora-endpoint> \
      -U <db-user> \
      -d <database>
    ```

    Use read replica for queries. Never run writes directly in prod.
  entry_type: runbook
  roles: [dev]
  tags: [database, production, postgres, aurora]
  source_file: apps/api/core/database
  confidence: 1.0

- id: runbook-db-003
  content: |
    **Seed Database**

    ```bash
    # Full seed (may fail on seedSets)
    pnpm seed

    # Workaround: seed specific domains
    pnpm seed:gallery
    pnpm seed:mocs

    # Direct SQL for specific records
    psql -c "INSERT INTO gallery_images (...) VALUES (...)"
    ```

    Known issue: seedSets() fails due to tags column type mismatch.
    Tech debt item - no active fix planned.
  entry_type: runbook
  roles: [dev]
  tags: [database, seed, data]
  source_file: apps/api/core/database/seeds
  confidence: 1.0

# =============================================================================
# INCIDENT RESPONSE
# =============================================================================

- id: runbook-incident-001
  content: |
    **API 5xx Errors Spike**

    1. Check CloudWatch Logs for error details
       ```
       aws logs filter-log-events \
         --log-group-name /aws/lambda/<function> \
         --filter-pattern "ERROR"
       ```

    2. Check Lambda metrics: duration, errors, throttles

    3. Common causes:
       - Database connection exhausted → check Aurora connections
       - Cold start timeouts → increase memory/timeout
       - Dependency failure → check S3, Cognito status

    4. If unrecoverable: rollback to previous version
  entry_type: runbook
  roles: [dev]
  tags: [incident, errors, api, troubleshooting]
  source_file: apps/api/core/observability
  confidence: 1.0

- id: runbook-incident-002
  content: |
    **Database Connection Issues**

    1. Check Aurora status in AWS Console

    2. Check connection count
       ```sql
       SELECT count(*) FROM pg_stat_activity;
       ```

    3. Kill idle connections if needed
       ```sql
       SELECT pg_terminate_backend(pid)
       FROM pg_stat_activity
       WHERE state = 'idle'
       AND query_start < now() - interval '10 minutes';
       ```

    4. If Aurora scaled to 0: first request triggers scale-up (30-60s delay)

    5. Increase min ACU if cold starts unacceptable
  entry_type: runbook
  roles: [dev]
  tags: [incident, database, connections, aurora]
  source_file: apps/api/core/database
  confidence: 1.0

- id: runbook-incident-003
  content: |
    **S3 Upload Failures**

    1. Check S3 bucket status and permissions

    2. Verify presigned URL generation
       ```bash
       # Test presign locally
       curl -X PUT "<presigned-url>" \
         -H "Content-Type: application/pdf" \
         --data-binary @test.pdf
       ```

    3. Check CORS configuration on bucket

    4. Common causes:
       - Expired presigned URL (1hr TTL)
       - Wrong Content-Type header
       - File too large (check maxSize in presign)
       - Bucket policy changed

    5. Orphaned uploads cleaned by lifecycle policy (7 days)
  entry_type: runbook
  roles: [dev]
  tags: [incident, s3, upload, troubleshooting]
  source_file: apps/api/core/storage
  confidence: 1.0

# =============================================================================
# MAINTENANCE
# =============================================================================

- id: runbook-maint-001
  content: |
    **Prune Old Lambda Versions**

    Serverless plugin handles this automatically:
    - Production: keeps 5 versions
    - Staging: keeps 3 versions

    Manual prune if needed:
    ```bash
    cd apps/api
    pnpm sls prune --stage production --number 5
    ```
  entry_type: runbook
  roles: [dev]
  tags: [maintenance, lambda, cleanup]
  source_file: apps/api/serverless.yml
  confidence: 1.0

- id: runbook-maint-002
  content: |
    **Clean Orphaned S3 Objects**

    S3 lifecycle policy auto-deletes objects in `uploads/temp/` after 7 days.

    Manual cleanup:
    ```bash
    # List orphaned objects (no corresponding DB record)
    aws s3 ls s3://bucket/uploads/temp/ --recursive

    # Delete specific prefix
    aws s3 rm s3://bucket/uploads/temp/2026-01-01/ --recursive
    ```

    Never delete from `uploads/final/` without DB verification.
  entry_type: runbook
  roles: [dev]
  tags: [maintenance, s3, cleanup, storage]
  source_file: apps/api/core/storage
  confidence: 1.0

- id: runbook-maint-003
  content: |
    **Update Dependencies**

    ```bash
    # Check outdated
    pnpm outdated

    # Update specific package
    pnpm update <package> --recursive

    # Update all (careful!)
    pnpm update --recursive

    # After update
    pnpm install
    pnpm lint:all && pnpm check-types:all && pnpm test:all
    ```

    Major version updates: test thoroughly in staging first.
    React, Drizzle, Serverless Framework updates need extra care.
  entry_type: runbook
  roles: [dev]
  tags: [maintenance, dependencies, updates]
  source_file: package.json
  confidence: 1.0

# =============================================================================
# MONITORING
# =============================================================================

- id: runbook-monitor-001
  content: |
    **Key Metrics to Watch**

    Lambda:
    - Duration p99 < 3s (cold start can be 5s)
    - Error rate < 1%
    - Throttles = 0

    Aurora:
    - Connection count < 80% of max
    - CPU < 80%
    - ACU scaling events

    S3:
    - 4xx errors (usually client-side, not critical)
    - 5xx errors (investigate immediately)

    CloudWatch dashboard: `/dashboards/lego-moc-prod`
  entry_type: runbook
  roles: [dev]
  tags: [monitoring, metrics, cloudwatch]
  source_file: apps/api/core/observability
  confidence: 1.0

- id: runbook-monitor-002
  content: |
    **Set Up Alerts**

    Critical (page immediately):
    - API error rate > 5% for 5 minutes
    - Aurora connection failures
    - Lambda duration p99 > 10s

    Warning (review next business day):
    - Error rate > 1% for 15 minutes
    - Aurora ACU at max for 30 minutes
    - S3 5xx errors

    Configure in CloudWatch Alarms or third-party (Datadog, etc.)
  entry_type: runbook
  roles: [dev]
  tags: [monitoring, alerts, cloudwatch]
  source_file: apps/api/core/observability
  confidence: 1.0
