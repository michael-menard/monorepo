---
doc_type: story
title: "WISH-20260: Automatic Retry Mechanism for Failed Flag Schedules"
story_id: WISH-20260
story_prefix: WISH
status: ready-for-qa
follow_up_from: WISH-2119
created_at: "2026-02-07T18:27:00-07:00"
updated_at: "2026-02-08T12:40:00-07:00"
depends_on: [WISH-2119]
estimated_points: 5
experiment_variant: control
---

# WISH-20260: Automatic Retry Mechanism for Failed Flag Schedules

## Follow-up Context

**Parent Story:** WISH-2119
**Source:** QA Discovery Notes - Enhancement Opportunity #4
**Original Finding:** "Automatic retry for failed schedules - Manual intervention required for failed schedules in MVP"
**Category:** Enhancement Opportunity
**Impact:** Medium
**Effort:** Medium

## Context

WISH-2119 implements flag scheduling infrastructure where schedules can fail during processing (e.g., database errors, network issues, cache invalidation failures). Currently, failed schedules require manual admin intervention to retry - admins must inspect CloudWatch logs, identify the failure reason, and manually create a new schedule or update the flag directly.

This creates operational burden and risks delays in critical flag updates (e.g., holiday promotions that fail to auto-enable, maintenance windows that fail to disable features). An automatic retry mechanism with exponential backoff would improve reliability and reduce admin toil.

## Goal

Automatically retry failed flag schedules with exponential backoff and configurable retry limits to improve reliability of scheduled flag updates without manual intervention.

## Non-goals

- **Manual retry endpoint** - admins can already recreate schedules manually
- **Retry for cancelled schedules** - cancellation is intentional, no retry needed
- **Retry alerting/notifications** - defer to future observability story
- **Retry metrics dashboard** - defer to future monitoring story
- **Custom retry policies per schedule** - single global retry policy in MVP

## Scope

### Endpoints Affected

**No new endpoints** - retry logic integrated into existing cron job from WISH-2119

**Reused from WISH-2119:**
- Cron job: `apps/api/lego-api/jobs/process-flag-schedules.ts` (enhanced with retry logic)
- Schedule repository for updating retry metadata

### Packages Affected

1. **Backend - Cron Job**: `apps/api/lego-api/jobs/`
   - `process-flag-schedules.ts` - Add retry logic to schedule processing (modified)

2. **Database Schema**: `packages/backend/database-schema/`
   - `src/schema/feature-flags.ts` - Add retry columns to feature_flag_schedules table (modified)
   - `src/migrations/app/` - Migration for retry columns (new)

3. **Backend - Config Domain**: `apps/api/lego-api/domains/config/`
   - `adapters/schedule-repository.ts` - Update methods to handle retry metadata (modified)

### Infrastructure Impact

**Database Schema Changes:**
```sql
ALTER TABLE feature_flag_schedules
  ADD COLUMN retry_count INTEGER DEFAULT 0 NOT NULL,
  ADD COLUMN max_retries INTEGER DEFAULT 3 NOT NULL,
  ADD COLUMN next_retry_at TIMESTAMP WITH TIME ZONE,
  ADD COLUMN last_error TEXT;

CREATE INDEX idx_schedules_next_retry_at ON feature_flag_schedules(next_retry_at);
```

**Retry Policy Configuration:**
- Max retries: 3 attempts (configurable via environment variable `FLAG_SCHEDULE_MAX_RETRIES`)
- Backoff strategy: Exponential backoff with jitter
  - Retry 1: 2 minutes + jitter (0-30 seconds)
  - Retry 2: 4 minutes + jitter (0-30 seconds)
  - Retry 3: 8 minutes + jitter (0-30 seconds)
- After max retries exceeded: status remains 'failed', manual intervention required

**Cron Job Changes:**
- Query includes schedules WHERE (status = 'pending' AND scheduled_at <= NOW()) OR (status = 'failed' AND next_retry_at <= NOW() AND retry_count < max_retries)
- On failure: Calculate next_retry_at using exponential backoff, increment retry_count, log retry attempt
- After max retries: Set status = 'failed' permanently, clear next_retry_at

## Acceptance Criteria

### Retry Logic

**AC1**: Database schema migration
- Migration adds 4 columns to feature_flag_schedules table: retry_count (default 0), max_retries (default 3), next_retry_at (nullable), last_error (nullable TEXT)
- Index created on next_retry_at for efficient cron job queries
- Migration applied before deploying enhanced cron job

**AC2**: Failed schedule retry query
- Cron job query extended to include failed schedules ready for retry:
  - WHERE (status = 'pending' AND scheduled_at <= NOW()) OR (status = 'failed' AND next_retry_at <= NOW() AND retry_count < max_retries)
- Order by next_retry_at ASC (retry failed schedules before new pending schedules)
- Limit: 100 schedules per execution (from WISH-2119)

**AC3**: Exponential backoff calculation
- On schedule processing failure:
  - Calculate backoff_minutes = 2^(retry_count + 1) (2, 4, 8 minutes)
  - Add jitter: random(0, 30) seconds
  - Set next_retry_at = NOW() + backoff_minutes + jitter
  - Increment retry_count
  - Update last_error with error message
- Zod utility function: `calculateNextRetryAt(retryCount: number): Date`

**AC4**: Retry attempt logging
- Log structured CloudWatch logs on retry attempts:
  - `{ level: "info", message: "Retrying failed schedule", scheduleId, flagKey, retryCount, nextRetryAt }`
- Log on final failure (max retries exceeded):
  - `{ level: "error", message: "Schedule retry limit exceeded", scheduleId, flagKey, retryCount, lastError }`

**AC5**: Max retries enforcement
- After retry_count >= max_retries:
  - Status remains 'failed'
  - Clear next_retry_at (schedule will not be retried again)
  - Log final failure to CloudWatch
  - Manual intervention required

**AC6**: Successful retry handling
- On successful retry:
  - Set status = 'applied'
  - Set applied_at = NOW()
  - Log success: `{ message: "Schedule applied on retry", scheduleId, flagKey, retryCount }`
  - Clear next_retry_at

**AC7**: Configurable max retries
- Environment variable: `FLAG_SCHEDULE_MAX_RETRIES` (default: 3)
- max_retries column in database allows per-schedule override (use case: critical schedules with higher retry limits)
- Validation: max_retries must be >= 0 and <= 10

### Testing

**AC8**: Unit tests for retry logic (minimum 5 tests)
- Test: calculateNextRetryAt returns correct backoff times (2, 4, 8 minutes)
- Test: Jitter is within 0-30 seconds range
- Test: Failed schedule increments retry_count and sets next_retry_at
- Test: Successful retry clears next_retry_at and sets status = 'applied'
- Test: Max retries exceeded clears next_retry_at and keeps status = 'failed'

**AC9**: Integration tests for cron job retry (minimum 3 tests)
- Test: Create schedule, simulate failure, verify retry_count incremented and next_retry_at set
- Test: Wait for next_retry_at, invoke cron job, simulate success, verify status = 'applied'
- Test: Simulate 3 failures, verify status = 'failed' permanently after max retries

**AC10**: Edge case tests
- Test: Concurrent retries - two cron jobs process same failed schedule, verify processed once (row locking from WISH-2119)
- Test: Schedule with retry_count = 2 fails again, verify next_retry_at = NOW() + 8 minutes
- Test: Schedule with custom max_retries = 5 retries 5 times before final failure

## Reuse Plan

### Existing Components (Extended)
- Cron job handler from WISH-2119 (`process-flag-schedules.ts`)
- Schedule repository from WISH-2119 (`schedule-repository.ts`)
- Row-level locking from WISH-2119 (`FOR UPDATE SKIP LOCKED`)
- CloudWatch structured logging from WISH-2119
- Flag update logic from WISH-2009

### New Components
- `calculateNextRetryAt()` utility function (exponential backoff + jitter)
- Database migration for retry columns

### Existing Patterns
- Exponential backoff patterns (align with existing retry patterns in codebase if present)
- CloudWatch structured logging
- Environment variable configuration

## Architecture Notes

**Hexagonal Architecture Compliance:**
- Retry logic added to cron job handler (`jobs/process-flag-schedules.ts`)
- Schedule repository extended with retry metadata updates (`adapters/schedule-repository.ts`)
- No HTTP layer changes (backend-only enhancement)
- No business logic coupling (retry logic is infrastructure concern)

**No Architecture Violations:** Story extends existing cron job infrastructure following patterns from WISH-2119.

## Test Plan

### Scope Summary
- **Endpoints:** None (cron job only)
- **UI Touched:** No (backend only)
- **Data/Storage:** Yes (feature_flag_schedules table schema changes)
- **Infrastructure:** Cron job retry logic

### Happy Path Tests

1. **First retry after failure**: Create schedule, simulate failure, verify retry_count = 1 and next_retry_at = NOW() + 2 minutes
2. **Successful retry**: Wait for next_retry_at, invoke cron job, simulate success, verify status = 'applied'
3. **Multiple retries**: Simulate 2 failures, verify retry_count increments and backoff increases (2, 4 minutes)

### Error Cases

1. **Max retries exceeded**: Simulate 3 failures, verify status = 'failed' permanently and next_retry_at cleared
2. **Invalid max_retries config**: Set FLAG_SCHEDULE_MAX_RETRIES = 15, verify validation error or capped at 10
3. **Database error during retry update**: Simulate error while updating retry metadata, verify error logged

### Edge Cases

1. **Concurrent retry processing**: Two cron jobs process same failed schedule, verify processed once (row locking)
2. **Retry at minute boundary**: Set next_retry_at to exact minute, verify processed within 60 seconds
3. **Schedule with custom max_retries**: Create schedule with max_retries = 5, verify retries 5 times
4. **Jitter distribution**: Run 100 retry calculations, verify jitter is evenly distributed between 0-30 seconds
5. **Exponential backoff overflow**: Test retry_count = 10, verify backoff capped at reasonable limit (e.g., 60 minutes)

### Required Tooling Evidence

**Backend Integration Tests:**
- Cron job retry: Create schedule, simulate failure, assert retry metadata updated
- Successful retry: Wait for retry window, invoke cron, assert status = 'applied'
- Max retries: Simulate 3+ failures, assert final failure state

**Backend Unit Tests:**
- calculateNextRetryAt: Assert backoff times (2, 4, 8 minutes)
- Jitter calculation: Assert range 0-30 seconds
- Retry metadata updates: Assert retry_count increment, next_retry_at calculation

**Frontend Tests:**
- N/A (no user-facing UI)

## Risks / Edge Cases

### MVP-Critical Risks

1. **Exponential backoff timing**
   - Risk: Retries may occur up to 60 seconds later due to cron frequency (1 minute)
   - Mitigation: Acceptable delay for retry use case, document in admin notes
   - Severity: Low

2. **Max retries too low for transient errors**
   - Risk: 3 retries may be insufficient for long-lived database outages
   - Mitigation: Make max_retries configurable via environment variable, document override process
   - Severity: Low (admins can manually retry after max attempts)

3. **Retry backlog accumulation**
   - Risk: Large number of failed schedules may exhaust 100-schedule limit per cron execution
   - Mitigation: Prioritize retries by next_retry_at ASC, monitor CloudWatch metrics for backlog
   - Severity: Medium (defer monitoring dashboard to future story)

4. **Permanent database failures**
   - Risk: Retries will fail indefinitely if underlying database issue persists
   - Mitigation: CloudWatch logs alert admins to repeated failures, max retries prevent infinite retries
   - Severity: Low (operations team handles database incidents separately)

5. **Jitter edge case (midnight boundary)**
   - Risk: next_retry_at calculation at midnight may cross day boundary unexpectedly
   - Mitigation: Use UTC timestamps consistently (inherited from WISH-2119)
   - Severity: Low

## Open Questions

None - all decisions finalized for MVP scope.

## Definition of Done

- [ ] All 10 Acceptance Criteria pass
- [ ] Backend: 5+ unit tests pass (exponential backoff, retry logic)
- [ ] Backend: 3+ integration tests pass (cron job retry, max retries, concurrent retries)
- [ ] Database migration created and applied (retry columns added)
- [ ] Cron job Lambda deployed with enhanced retry logic
- [ ] TypeScript compilation passes
- [ ] ESLint passes
- [ ] Code review approved
- [ ] CloudWatch logs confirm retry attempts logged correctly

## Follow-up Stories (Future)

### Phase 4+
- Retry metrics dashboard (CloudWatch metrics for retry success/failure rates)
- Retry alerting (SNS notifications when schedule exceeds max retries)
- Custom retry policies per schedule (configurable backoff strategy per schedule)
- Retry preview endpoint (simulate retry timeline for failed schedules)

### Integration with WISH-2119
- Schedule cleanup cron job should consider retry state (don't purge schedules mid-retry)

## QA Discovery Notes (Auto-Generated)

_Added by Autonomous Elaboration on 2026-02-08_

### MVP Gaps Resolved

No MVP-critical gaps identified. All audit checks passed.

| # | Finding | Resolution | Status |
|---|---------|------------|--------|
| — | None | — | — |

### Non-Blocking Items (Logged to KB)

| # | Finding | Category | Status |
|---|---------|----------|--------|
| 1 | Retry alerting/notifications | observability | Deferred to Phase 4+ |
| 2 | Retry metrics dashboard | observability | Deferred to Phase 4+ |
| 3 | Manual retry endpoint | enhancement | Deferred to Phase 4+ |
| 4 | Retry history tracking | enhancement | Deferred to Phase 5+ |
| 5 | Exponential backoff cap specification | implementation-note | Add 60-minute cap in implementation |
| 6 | Custom retry policies per schedule | enhancement | Deferred to Phase 4+ |
| 7 | Retry preview/dry-run endpoint | enhancement | Deferred to Phase 4+ |
| 8 | Adaptive backoff based on error type | enhancement | Deferred to Phase 5+ |
| 9 | Retry priority queuing | enhancement | Deferred to Phase 4+ |
| 10 | Circuit breaker for database failures | enhancement | Deferred to Phase 5+ |
| 11 | Retry batch optimization | enhancement | Deferred to Phase 5+ |

### Summary

- ACs added: 0
- Non-blocking items logged: 11
- Mode: autonomous
- Verdict: PASS

All 8 audit checks passed. Story is ready for implementation. Core retry journey specified in ACs 1-10. Database migration, exponential backoff calculation, max retries enforcement, and comprehensive testing specified.
