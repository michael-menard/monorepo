---
doc_type: story
title: "WISH-20320: Redis Cluster mode for high availability (multi-AZ failover, load balancing)"
story_id: WISH-20320
story_prefix: WISH
status: pending
follow_up_from: WISH-2124
phase: 3
created_at: "2026-01-30T23:30:00-07:00"
updated_at: "2026-01-30T23:30:00-07:00"
depends_on: [WISH-2124]
estimated_points: 5
sizing_warning: false
priority: P2
complexity: High
---

# WISH-20320: Redis Cluster mode for high availability (multi-AZ failover, load balancing)

## Follow-up Context

**Parent Story**: WISH-2124
**Source**: QA Discovery Notes - Enhancement Opportunity #1
**Original Finding**: Redis Cluster mode for high availability (multi-AZ failover, load balancing)
**Category**: Enhancement Opportunity
**Impact**: High (production availability and fault tolerance)
**Effort**: High (infrastructure redesign and migration)

### Background

WISH-2124 implemented single-instance ElastiCache Redis for feature flag caching, which is sufficient for MVP workloads. However, single-instance Redis has critical availability limitations for production systems:

1. **Single point of failure**: If the Redis instance fails, all cache operations fail until manual intervention
2. **No automatic failover**: Instance failures require manual recovery, potentially causing extended downtime
3. **AZ-level failures**: Single-AZ deployment is vulnerable to availability zone outages
4. **Limited scalability**: Single instance has fixed memory and throughput limits

Redis Cluster mode provides multi-AZ deployment with automatic failover, distributing cache across multiple nodes for high availability and horizontal scalability.

**Production Readiness Signal:**
This enhancement becomes critical when:
- SLA requirements demand >99.9% uptime
- Cache unavailability impacts revenue or critical user flows
- Traffic patterns show consistent cache hit rates >90% (cache is load-bearing)
- Single instance memory utilization consistently >60%

## Context

WISH-2124 uses ElastiCache single-instance Redis (`cache.t3.micro`, single-AZ) for feature flag caching. While this provides distributed caching across Lambda instances, it lacks production-grade availability guarantees:

**Current Architecture Limitations:**
- **No automatic failover**: Instance failures require manual intervention (5-15 minute recovery time)
- **Single-AZ vulnerability**: AZ-level outages cause complete cache loss
- **Manual scaling**: Memory exhaustion requires instance resize with downtime
- **No read replicas**: All read/write traffic hits single instance

**Redis Cluster Mode Benefits:**
- **Multi-AZ deployment**: Primary and replica nodes across 2-3 availability zones
- **Automatic failover**: Replica promotion on primary failure (typically <60 seconds)
- **Horizontal scaling**: Add/remove nodes without downtime
- **Read replicas**: Distribute read traffic across replicas for load balancing
- **Data sharding**: Distribute data across multiple primary nodes for memory scaling

**Cost vs. Availability Tradeoff:**
- Single-instance: ~$15-30/month, no HA
- Cluster mode (3 nodes): ~$45-90/month, 99.9%+ availability
- Reserved capacity (1-year): ~40% cost savings for production workloads

## Goal

Migrate ElastiCache from single-instance to Cluster mode with multi-AZ deployment for production-grade high availability. Enable automatic failover, horizontal scaling, and fault tolerance for feature flag caching infrastructure.

**Success Criteria:**
- Automatic failover completes within 60 seconds (replica promotion)
- Cache availability >99.9% over 30-day rolling window
- Zero data loss during failover (replica sync lag <500ms)
- Read traffic distributed across replica nodes (load balancing)
- Transparent migration with zero downtime for API clients

## Non-goals

- **Multi-region replication**: Global cache distribution deferred to separate story
- **Cache warming on deployment**: Pre-population strategy deferred (relies on lazy loading)
- **Cache analytics dashboard**: CloudWatch metrics sufficient for MVP
- **Reserved capacity planning**: Initial deployment uses on-demand pricing
- **Data sharding across primaries**: Single primary with replicas sufficient for MVP flag count
- **Redis Sentinel**: ElastiCache Cluster mode uses built-in automatic failover (no manual Sentinel setup)
- **Connection pooling changes**: Existing `ioredis` client from WISH-2124 supports cluster mode transparently

## Scope

### Packages Affected

**Backend Infrastructure:**
- `apps/api/lego-api/core/cache/redis-client.ts` - Update Redis client initialization for cluster mode
- `apps/api/lego-api/domains/config/adapters/RedisCacheAdapter.ts` - No code changes (cluster mode transparent via client)
- Infrastructure code (CDK/Terraform): ElastiCache cluster mode configuration, VPC subnet groups

**Environment Configuration:**
- `.env` files: Update `REDIS_URL` to cluster endpoint format
- AWS Secrets Manager: Store cluster configuration endpoint
- Lambda environment variables: Inject cluster-aware connection string

**Docker Compose (Local Development):**
- `apps/api/lego-api/docker-compose.yml` - Add Redis Cluster mode simulation (6-node local cluster)

### Infrastructure Components

**AWS ElastiCache Cluster Mode:**
- **Node type**: `cache.t3.micro` (3 nodes: 1 primary, 2 replicas)
- **Engine**: Redis 7.x (cluster mode enabled)
- **Availability Zones**: 3 AZs (us-east-1a, us-east-1b, us-east-1c)
- **Automatic failover**: Enabled (60-second recovery time objective)
- **Backup retention**: 1 day (point-in-time recovery)
- **Multi-AZ**: Enabled
- **Estimated cost**: ~$45-90/month (3x single-instance cost)

**Network Configuration:**
- **Subnet Groups**: Span 3 private subnets across availability zones
- **Security Groups**: Same rules as WISH-2124 (Lambda → ElastiCache port 6379)
- **Endpoint**: Cluster configuration endpoint (e.g., `my-cluster.abc123.clustercfg.use1.cache.amazonaws.com:6379`)

**Lambda Configuration:**
- **VPC attachment**: No changes (same VPC as WISH-2124)
- **Environment variable**: Update `REDIS_URL` to cluster endpoint format
- **Client library**: `ioredis` v5.x already supports cluster mode

**Local Development:**
- Docker Compose: 6-node Redis Cluster (3 primaries + 3 replicas)
- Tool: `redis-cluster-docker` for local testing
- Port mapping: `7000-7005:7000-7005`

### Endpoints Impacted

**No API contract changes** - this is a transparent infrastructure upgrade.

Existing endpoints continue working unchanged:
- `GET /api/config/flags/:flagKey` - Now uses cluster-mode Redis (client handles node routing)
- `PATCH /api/admin/flags/:flagKey` - Invalidates cache across cluster nodes
- `GET /api/config/flags` - Cluster-aware cache operations

## Acceptance Criteria

### AC 1: ElastiCache Cluster Mode Provisioning
**Given** the need for high availability caching infrastructure
**When** provisioning ElastiCache cluster
**Then** create cluster with 3 nodes (1 primary, 2 replicas)
**And** distribute nodes across 3 availability zones (multi-AZ enabled)
**And** enable automatic failover with <60 second recovery time
**And** set node type to `cache.t3.micro` (same instance size as WISH-2124)

**Evidence:**
- AWS CLI: `describe-cache-clusters` shows `CacheClusterStatus: available`, `NumCacheNodes: 3`
- AWS Console: Cluster topology shows 1 primary + 2 replicas across 3 AZs
- CloudFormation/CDK synthesized template includes cluster configuration
- Cost Explorer: ElastiCache charges reflect 3-node cluster pricing (~$45-90/month)

### AC 2: Redis Client Cluster Mode Configuration
**Given** the need to connect Lambda to Redis Cluster
**When** initializing the Redis client in `core/cache/redis-client.ts`
**Then** configure `ioredis` with cluster-aware connection options
**And** use cluster configuration endpoint (not individual node endpoints)
**And** enable `redisOptions.enableReadyCheck` for failover detection
**And** set `clusterRetryStrategy` with exponential backoff (3 retries max)

**Evidence:**
- `redis-client.ts` instantiates `Redis.Cluster` with configuration endpoint
- Unit tests verify cluster options: `enableReadyCheck: true`, `retryStrategy` defined
- Integration tests connect to local Redis Cluster (Docker Compose)
- CloudWatch logs: "Connected to Redis Cluster" on Lambda cold start

**Example Configuration:**
```typescript
import Redis from 'ioredis'

export const createRedisClusterClient = (clusterUrl: string): Redis.Cluster => {
  return new Redis.Cluster(
    [{ host: clusterUrl, port: 6379 }],
    {
      redisOptions: {
        enableReadyCheck: true,
        maxRetriesPerRequest: 3,
      },
      clusterRetryStrategy(times) {
        const delay = Math.min(times * 100, 2000)
        return delay
      },
      enableOfflineQueue: false, // Fail fast on cluster unavailable
    }
  )
}
```

### AC 3: Automatic Failover Validation
**Given** Redis Cluster with 1 primary and 2 replicas
**When** the primary node fails (simulated via `CLUSTER FAILOVER` command)
**Then** ElastiCache automatically promotes a replica to primary within 60 seconds
**And** existing Lambda connections reconnect to new primary automatically
**And** API requests continue succeeding during failover (no 500 errors)
**And** CloudWatch logs show "Redis reconnected to new primary"

**Evidence:**
- Integration test: Trigger failover via Redis CLI, measure recovery time (<60s)
- Load test: Continuous requests during failover show <1% error rate
- CloudWatch logs: "Replica promoted to primary at <timestamp>"
- No client-facing errors during failover window

### AC 4: Multi-AZ Resilience
**Given** Redis Cluster nodes distributed across 3 availability zones
**When** an AZ-level failure occurs (simulated by stopping nodes in one AZ)
**Then** remaining nodes in other AZs continue serving requests
**And** automatic failover promotes replica from different AZ if primary affected
**And** API availability maintained (cache hit rate may temporarily drop)

**Evidence:**
- AWS Console: Cluster topology shows nodes in 3 different subnets/AZs
- Integration test: Simulate AZ failure, verify cache continues operating
- CloudWatch metrics: Cache hit rate dips but recovers within 5 minutes
- No production incidents during AZ outages

### AC 5: Read Replica Load Balancing
**Given** Redis Cluster with 2 replica nodes
**When** Lambda makes read-only cache requests (`GET` operations)
**Then** distribute read traffic across primary and replicas
**And** achieve ~50% read traffic on replicas (load balancing)
**And** reduce primary node load for better write performance

**Evidence:**
- CloudWatch metrics: Replica CPU utilization >20% (traffic distributed)
- `ioredis` configuration: `scaleReads: 'slave'` or `scaleReads: 'all'`
- Integration tests verify reads hit replicas (inspect client connection stats)
- Primary node CPU utilization lower than single-instance baseline

### AC 6: Zero-Downtime Migration from Single-Instance
**Given** production running single-instance Redis from WISH-2124
**When** migrating to cluster mode
**Then** use blue-green deployment strategy (provision cluster, switch traffic)
**And** validate cluster in staging before production cutover
**And** implement rollback plan (revert `REDIS_URL` to single-instance endpoint)
**And** complete migration with zero API downtime

**Evidence:**
- Deployment runbook documents blue-green migration steps
- Staging validation: All integration tests pass against cluster
- Production cutover: API metrics show no error rate spike
- Rollback tested in staging (revert to single-instance in <5 minutes)

### AC 7: Cluster Health Monitoring
**Given** Redis Cluster deployed in production
**When** monitoring cluster health
**Then** publish CloudWatch metrics for cluster status
**And** alert on `ReplicationLag > 500ms` (replica sync lag)
**And** alert on `CacheHitRate < 80%` (cache degradation)
**And** alert on `CPUUtilization > 75%` (scaling threshold)

**Evidence:**
- CloudWatch dashboard includes cluster-specific metrics (replication lag, node status)
- CloudWatch alarms configured for critical thresholds
- PagerDuty/SNS integration for production alerts
- Monthly review: Cluster health metrics reviewed and tuned

### AC 8: Backup and Recovery Configuration
**Given** the need for data recovery capabilities
**When** configuring ElastiCache cluster
**Then** enable automatic backups with 1-day retention
**And** schedule daily backup window during low-traffic hours (2-4 AM UTC)
**And** document restore procedure (create new cluster from snapshot)

**Evidence:**
- ElastiCache cluster configuration: `SnapshotRetentionLimit: 1`
- AWS CLI: `describe-cache-clusters` shows `PreferredMaintenanceWindow: 02:00-04:00`
- Runbook: Disaster recovery procedure for restoring from snapshot
- Test: Create snapshot, restore to new cluster, validate data integrity

### AC 9: Connection String Migration
**Given** existing single-instance `REDIS_URL` in Lambda environment variables
**When** deploying cluster mode
**Then** update `REDIS_URL` to cluster configuration endpoint
**And** support environment-specific endpoints (staging vs. production)
**And** fetch endpoint from AWS Secrets Manager at runtime

**Evidence:**
- Lambda environment variable: `REDIS_URL_CLUSTER` set to cluster endpoint
- Secrets Manager: Secrets for `redis/cluster/staging` and `redis/cluster/production`
- `redis-client.ts` reads from Secrets Manager or environment variable (fallback)
- Integration tests verify connection string parsing

### AC 10: Local Development Cluster Simulation
**Given** developers need to test cluster mode locally
**When** running Docker Compose for local development
**Then** provision 6-node Redis Cluster (3 primaries + 3 replicas)
**And** use `redis-cluster-docker` image for cluster setup
**And** expose ports 7000-7005 for cluster nodes
**And** update README with cluster setup instructions

**Evidence:**
- `docker-compose.yml` includes Redis Cluster service definition
- `docker-compose up redis-cluster` starts 6-node cluster successfully
- Local integration tests connect to cluster at `localhost:7000-7005`
- README section: "Redis Cluster Local Development Setup"

### AC 11: Cost Monitoring and Budgeting
**Given** cluster mode increases infrastructure cost (~3x single-instance)
**When** deploying to production
**Then** CloudWatch billing alarm triggers at $100/month threshold
**And** Cost Explorer tagged for cluster resources (`Service=ElastiCache-Cluster`, `Feature=FeatureFlags`)
**And** monthly cost review includes cluster right-sizing analysis

**Evidence:**
- CloudWatch billing alarm configured at $100/month
- Cost Explorer shows ElastiCache-Cluster line item (~$45-90/month for 3 nodes)
- Monthly review: Evaluate node utilization, consider reserved capacity
- Documentation: Cost optimization guide for cluster mode

### AC 12: Graceful Degradation on Cluster Unavailability
**Given** Redis Cluster may become temporarily unavailable
**When** all cluster nodes fail or network connectivity lost
**Then** Lambda falls back to database reads (same as WISH-2124)
**And** log error with structured logging: "cluster_unavailable=true"
**And** respond with 200 status (graceful degradation, not 500 error)

**Evidence:**
- Integration test: Stop all cluster nodes, verify database fallback
- CloudWatch logs: "Redis Cluster unavailable, falling back to database"
- API response time higher (~150-250ms) but requests succeed
- No client-facing errors during cluster outages

## Reuse Plan

### Packages to Reuse
- **`packages/backend/db`**: Database fallback on cluster failure (same pattern as WISH-2124)
- **`@repo/logger`**: Structured logging for cluster events (failover, replication lag)
- **`RedisCacheAdapter`**: From WISH-2124, no code changes required (cluster mode transparent)

### Patterns to Reuse
- **Graceful error handling**: From WISH-2124, database fallback on cache failure
- **Environment configuration**: Same `.env` and Secrets Manager patterns
- **Retry logic**: Exponential backoff from WISH-2124 (extended for cluster failover)
- **DI container pattern**: Wire cluster client in `apps/api/lego-api/domains/config/index.ts`

### Infrastructure Patterns
- **CDK/Terraform modules**: Extend ElastiCache module with cluster configuration
- **VPC setup**: Reuse VPC, subnet groups, security groups from WISH-2124
- **CloudWatch monitoring**: Extend existing dashboard with cluster metrics

## Architecture Notes

### Cluster Mode Architecture

**Node Distribution:**
```
AZ 1 (us-east-1a): Primary (read/write)
AZ 2 (us-east-1b): Replica 1 (read-only)
AZ 3 (us-east-1c): Replica 2 (read-only)
```

**Failover Flow:**
1. Primary node fails (AZ 1 outage)
2. ElastiCache detects failure (<10 seconds)
3. Promotes Replica 1 in AZ 2 to primary
4. `ioredis` client detects topology change
5. Reconnects to new primary automatically
6. Total recovery time: <60 seconds

**Read Load Balancing:**
- Write operations (`SET`, `DEL`): Always route to primary
- Read operations (`GET`, `HGETALL`): Distribute across primary + replicas
- `ioredis` configuration: `scaleReads: 'slave'` (prefer replicas for reads)

**Data Consistency:**
- Replication: Asynchronous (eventual consistency)
- Replica lag target: <500ms (monitored via CloudWatch)
- On failover: Potential data loss if lag >0 (acceptable for cache)

### Client Configuration

**Cluster Client Initialization:**
```typescript
// apps/api/lego-api/core/cache/redis-client.ts
import Redis from 'ioredis'

export const createRedisClusterClient = (clusterEndpoint: string): Redis.Cluster => {
  return new Redis.Cluster(
    [{ host: clusterEndpoint, port: 6379 }],
    {
      redisOptions: {
        enableReadyCheck: true,
        password: process.env.REDIS_PASSWORD, // If auth enabled
      },
      clusterRetryStrategy(times) {
        if (times > 3) return null // Stop retrying after 3 attempts
        return Math.min(times * 100, 2000) // Exponential backoff
      },
      scaleReads: 'slave', // Read from replicas, write to primary
      enableOfflineQueue: false, // Fail fast on cluster down
    }
  )
}
```

**DI Container Wiring:**
```typescript
// apps/api/lego-api/domains/config/index.ts
const redisClusterClient = createRedisClusterClient(process.env.REDIS_CLUSTER_URL!)
const cacheAdapter = new RedisCacheAdapter(redisClusterClient) // Same adapter from WISH-2124
const flagService = new FlagService(flagRepository, cacheAdapter)
```

**Backward Compatibility:**
- `RedisCacheAdapter` interface unchanged
- `ioredis` supports both single-instance and cluster modes transparently
- Feature flag service code unaware of cluster topology

## Test Plan

### Happy Path Tests

1. **Cluster Initialization**
   - Start Lambda with cluster endpoint → verify successful connection
   - Verify connection pool established across cluster nodes
   - Check CloudWatch logs: "Connected to Redis Cluster"

2. **Read/Write Operations**
   - Cache flag via `SET` → verify written to primary
   - Read flag via `GET` → verify read from replica (load balancing)
   - Invalidate cache via `DEL` → verify deleted from all nodes

3. **Automatic Failover**
   - Simulate primary failure → verify replica promotion (<60s)
   - Verify Lambda reconnects to new primary automatically
   - Verify API requests succeed during failover

4. **Multi-AZ Resilience**
   - Simulate AZ outage → verify remaining nodes continue serving
   - Verify cache hit rate recovers after failover

5. **Read Load Balancing**
   - Issue 100 concurrent reads → verify traffic distributed across replicas
   - Verify primary CPU utilization lower than single-instance baseline

### Error Cases

1. **All Nodes Unavailable**
   - Stop all cluster nodes → verify database fallback
   - Verify API responds 200 (graceful degradation)
   - Check logs: "cluster_unavailable=true"

2. **Replication Lag Spike**
   - Simulate high write load → monitor replication lag
   - Verify alerts trigger if lag >500ms
   - Verify read-after-write consistency not guaranteed (eventual consistency)

3. **Connection Pool Exhaustion**
   - Trigger 100 concurrent requests → verify cluster handles load
   - Verify no "connection pool exhausted" errors

4. **Cluster Configuration Endpoint Misconfigured**
   - Provide invalid endpoint → verify error logging and database fallback
   - Verify no 500 errors (graceful failure)

### Edge Cases

1. **Split-Brain Scenario**
   - Simulate network partition → verify cluster quorum logic
   - Verify only one primary exists (no split-brain)

2. **Backup and Restore**
   - Trigger manual backup → restore to new cluster
   - Verify cache data integrity after restore

3. **Rolling Node Replacement**
   - Replace nodes one-by-one (simulate maintenance)
   - Verify zero downtime during rolling replacement

4. **Cross-AZ Latency**
   - Measure replica sync latency (us-east-1a → us-east-1b)
   - Verify lag consistently <500ms

5. **Cost Spike Detection**
   - Increase cluster size to 6 nodes → verify billing alarm triggers

### Backend Testing Requirements

**Integration Tests:**
- Docker Compose: Spin up local 6-node Redis Cluster
- Test cases: All AC scenarios against local cluster
- Test data: 100+ feature flags cached across cluster

**Load Tests:**
- Artillery: 100 concurrent requests during failover simulation
- Metrics: P95 latency, error rate, cache hit rate
- Duration: 10 minutes sustained load

**.http Files (Manual Testing):**
```http
### Cache flag (write to primary)
PATCH {{host}}/api/admin/flags/wishlist-feature
Content-Type: application/json

{
  "isEnabled": true
}

### Get flag (read from replica)
GET {{host}}/api/config/flags/wishlist-feature

### List flags (cluster-aware bulk operation)
GET {{host}}/api/config/flags
```

**CloudWatch Queries:**
```
# Cache hit rate by node
fields @timestamp, cache_hit, node_id
| filter domain = "config"
| stats avg(cache_hit) as hit_rate by node_id

# Replication lag monitoring
fields @timestamp, replication_lag_ms
| filter replication_lag_ms > 500
```

## Risks & Mitigations

### Risk 1: Increased Infrastructure Cost
**Likelihood**: High
**Impact**: Medium (budget constraints)

**Mitigation:**
- Start with 3-node cluster (minimum for HA), not over-provisioned
- Cost Explorer tagging and billing alarms ($100/month threshold)
- Reserved capacity planning after 3 months (40% cost savings)
- Right-size instance types based on actual utilization

### Risk 2: Replication Lag Under High Write Load
**Likelihood**: Medium
**Impact**: Medium (stale cache reads)

**Mitigation:**
- Monitor replication lag via CloudWatch (alert if >500ms)
- Accept eventual consistency model for cache (data is in database)
- TTL ensures stale cache expires within 5 minutes (worst case)
- Consider read-after-write consistency bypass (read from primary for critical paths)

### Risk 3: Complex Failover Behavior
**Likelihood**: Medium
**Impact**: High (if failover fails)

**Mitigation:**
- Integration tests validate failover in staging before production
- Rollback plan: Revert to single-instance endpoint if failover issues
- ElastiCache automatic failover tested and proven (AWS-managed)
- Runbook documents failover scenarios and troubleshooting

### Risk 4: Client Library Compatibility Issues
**Likelihood**: Low
**Impact**: High (if client cannot connect to cluster)

**Mitigation:**
- `ioredis` v5.x supports cluster mode out-of-box (verified in docs)
- Integration tests validate cluster connection in staging
- Local Docker Compose cluster for pre-production testing
- Rollback plan: Revert to single-instance client configuration

### Risk 5: Network Latency Between AZs
**Likelihood**: Low
**Impact**: Low (slightly higher replication lag)

**Mitigation:**
- Accept cross-AZ latency (typically <2ms in same region)
- Monitor replication lag (<500ms target)
- ElastiCache optimizes cross-AZ replication (AWS-managed)

## Deployment Strategy

### Phase 1: Local Development Setup (Week 1)
1. Update `docker-compose.yml` with 6-node Redis Cluster
2. Update `redis-client.ts` for cluster mode support
3. Run integration tests against local cluster
4. Document developer onboarding for cluster setup

### Phase 2: Staging Cluster Deployment (Week 1-2)
1. Provision ElastiCache cluster in staging (3 nodes, multi-AZ)
2. Update Lambda environment variable: `REDIS_CLUSTER_URL`
3. Deploy Lambda with cluster client to staging
4. Run full test suite (unit, integration, load tests)
5. Validate failover behavior (simulate primary failure)

### Phase 3: Production Blue-Green Deployment (Week 2)
1. Provision ElastiCache cluster in production (3 nodes)
2. Parallel run: Single-instance + cluster (dual caching for validation)
3. Switch 10% traffic to cluster endpoint → monitor for 1 hour
4. If metrics pass (error rate <0.1%, hit rate >80%):
   - Promote to 50% → 100%
5. If any metric fails:
   - Rollback to single-instance endpoint

### Phase 4: Post-Deployment Monitoring (Week 2-3)
1. Monitor cluster health for 7 days:
   - Replication lag, cache hit rate, CPU utilization, error rate
2. Cost Explorer validation: Cluster charges match estimate (~$45-90/month)
3. Decommission single-instance Redis (cleanup)
4. Document lessons learned and update runbooks

## Related Stories

**Dependencies:**
- **WISH-2124**: Redis infrastructure setup and migration from in-memory cache (provides single-instance Redis baseline)

**Follow-ups:**
- **WISH-2126**: Cache warming strategy (pre-populate on cold start)
- **WISH-2127**: Multi-region Redis replication (global latency optimization)
- **WISH-2128**: Cache analytics dashboard (Grafana/Prometheus integration)

**Future Enhancements:**
- Reserved capacity planning for cost optimization (40% savings)
- Data sharding across multiple primaries (for >10,000 flags)
- Redis pub/sub for real-time flag update propagation

## Definition of Done

- [ ] All 12 acceptance criteria pass
- [ ] Unit tests pass (15+ tests, >80% coverage)
- [ ] Integration tests pass (20+ tests with Docker Compose cluster)
- [ ] Load tests pass (100 concurrent requests, <1% error rate during failover)
- [ ] Infrastructure tests pass (CDK/Terraform synthesize validates cluster config)
- [ ] Manual validation complete (Redis CLI cluster commands, AWS Console topology)
- [ ] Failover test successful (primary failure → replica promotion <60s)
- [ ] Production blue-green deployment successful (zero downtime)
- [ ] CloudWatch dashboard shows cluster metrics (replication lag, node status)
- [ ] Cost Explorer confirms cluster charges ~$45-90/month
- [ ] No customer-reported issues for 7 days post-deployment
- [ ] TypeScript compilation passes
- [ ] ESLint passes with no errors
- [ ] Code review approved
- [ ] Documentation updated (README, runbooks, architecture docs)

## Open Questions

None - story is ready for elaboration.

## Token Budget

### Phase Summary

| Phase | Estimated | Actual | Delta | Notes |
|-------|-----------|--------|-------|-------|
| Story Generation | ~8k | — | — | Follow-up from WISH-2124 |
| Elaboration | ~15k | — | — | Infrastructure design + testing strategy |
| Implementation | ~20k | — | — | Cluster setup + migration + validation |
| Code Review | ~8k | — | — | Infrastructure changes |
| **Total** | ~51k | — | — | High-complexity infrastructure story |

## Agent Log

| Timestamp (America/Denver) | Agent | Action | Outputs |
|---|---|---|---|
| 2026-01-30 23:30 | pm-story-followup-leader | Created follow-up from WISH-2124 finding #1 | WISH-20320.md (initial draft) |

---
