---
doc_type: story
title: "WISH-20330: Cache warming strategy (pre-populate on cold start, CloudWatch triggers)"
story_id: WISH-20330
story_prefix: WISH
status: backlog
follow_up_from: WISH-2124
created_at: "2026-01-30T00:00:00Z"
updated_at: "2026-01-30T00:00:00Z"
depends_on: [WISH-2124]
estimated_points: 3
priority: P2
complexity: Medium
---

# WISH-20330: Cache warming strategy (pre-populate on cold start, CloudWatch triggers)

## Follow-up Context

**Parent Story**: WISH-2124
**Source**: QA Discovery Notes - Enhancement Opportunity #2
**Original Finding**: Cache warming on deployment (pre-populating cache on cold start adds complexity. MVP relies on lazy population)
**Category**: Enhancement Opportunity
**Impact**: Medium (reduces cold start latency and improves cache hit rates)
**Effort**: Medium (requires infrastructure orchestration and cache preloading logic)

### Background

WISH-2124 implemented Redis-backed feature flag caching with lazy population (cache-on-read). While this approach is simple and works for MVP, it has performance limitations:

1. **Cold start cache misses**: First request after Lambda cold start always hits database (cache empty)
2. **Gradual warmup**: Cache hit rate starts at 0% and gradually increases as requests populate cache
3. **Deployment cache invalidation**: Every deployment flushes cache, requiring full re-warm
4. **Predictable latency spikes**: Known periods (post-deployment, cold starts) have degraded performance

This story implements proactive cache warming to pre-populate frequently accessed feature flags before user requests arrive, eliminating cold start cache misses and maintaining consistently high cache hit rates.

## Context

WISH-2124's lazy cache population (cache-on-read) works well for steady-state traffic but creates predictable performance degradation during:

1. **Lambda cold starts**: New Lambda instances start with empty Redis cache keys, requiring database reads for first N requests
2. **Post-deployment**: Cache flush on deployment means all instances gradually re-warm cache through user traffic
3. **Low-traffic periods**: Infrequently accessed flags expire from cache, requiring database reads when next requested

Cache warming proactively loads critical feature flags into Redis before user requests, providing:

- **Consistent performance**: P95 latency < 50ms for warmed flags (vs 100-200ms database reads)
- **Higher cache hit rates**: Target 95%+ (vs 80% with lazy loading)
- **Reduced database load**: Fewer SELECT queries on `feature_flags` table
- **Better user experience**: No latency spikes after deployments or cold starts

**Key Dependencies:**
- WISH-2124 must be complete with stable Redis infrastructure and `RedisCacheAdapter`
- Feature flag endpoints must be in production with measurable cache metrics

**Production Readiness Signal:**
This story becomes critical when:
- Post-deployment cache hit rate < 50% for first 5 minutes
- P95 latency > 150ms during cold start periods
- Database query load spikes > 100 QPS after deployments

## Goal

Implement proactive cache warming for feature flags to eliminate cold start cache misses and maintain consistently high cache hit rates across deployments and Lambda scaling events.

**Success Criteria:**
- Cache hit rate > 95% within 30 seconds of deployment or cold start
- P95 latency < 50ms for warmed flags (down from 100-200ms database reads)
- Database query load reduced by 70%+ during warmup periods
- Zero user-facing errors during cache warming process

## Non-goals

- **Warming all flags**: Only warm frequently accessed flags (top 20 by request volume)
- **Real-time cache invalidation**: Pub/sub cache updates deferred to future story (WISH-2129)
- **Multi-region cache warming**: Global warming deferred to multi-region story
- **Custom warmup schedules**: MVP uses fixed schedule (on deployment, hourly)
- **Cache warming for other domains**: This story is feature flags only (other domains can reuse pattern)
- **Predictive warming**: ML-based prediction of flags to warm deferred
- **User-specific cache warming**: MVP warms shared flags only (user targeting deferred)

## Scope

### Packages Affected

**Backend Cache Warming:**
- `apps/api/lego-api/domains/config/application/cache-warmer.ts` - New cache warming service
- `apps/api/lego-api/domains/config/application/feature-flag-service.ts` - Add `warmCache()` method
- `apps/api/lego-api/domains/config/adapters/redis-cache-adapter.ts` - Add `setMany()` batch operation
- `apps/api/lego-api/core/cache/redis-client.ts` - Add pipeline support for bulk operations

**Infrastructure:**
- `apps/api/lego-api/infrastructure/lambdas/warm-cache-handler.ts` - New Lambda handler for scheduled warming
- CloudWatch Events: EventBridge schedule rule (hourly trigger)
- Lambda post-deployment hook: Trigger warming after deployment completes

**Configuration:**
- `.env` files: Add `CACHE_WARMING_ENABLED=true`, `CACHE_WARMING_TOP_N=20`
- Environment variables: Configure warming behavior per environment

### Endpoints Impacted

**New Endpoint:**
- `POST /api/admin/cache/warm` - Manual cache warming trigger (admin only)
  - Request: `{ flagKeys?: string[] }` (optional specific flags, defaults to top 20)
  - Response: `{ warmed: number, duration: number }`

**Existing Endpoints (Unchanged):**
- `GET /api/config/flags/:flagKey` - Benefits from warmed cache (higher hit rate)
- `PATCH /api/admin/flags/:flagKey` - Continues to invalidate cache after update

### Infrastructure Components

**EventBridge Schedule Rule:**
- Rule name: `feature-flags-cache-warming`
- Schedule: `rate(1 hour)` (every hour)
- Target: `warm-cache-handler` Lambda function
- Payload: `{ triggerType: "scheduled" }`

**Lambda Post-Deployment Hook:**
- CDK/Terraform: Add `on-deploy` trigger for `warm-cache-handler`
- Trigger timing: After deployment completes, before traffic shift
- Timeout: 30 seconds max (warming should complete within 10s)

**CloudWatch Metrics (New):**
- `cache_warming_duration`: Time to warm cache (target: <5 seconds)
- `cache_warming_flags_count`: Number of flags warmed (target: 20)
- `cache_warming_errors`: Errors during warming (alert threshold: >1)

## Acceptance Criteria

### AC 1: CacheWarmer Service Implementation
**Given** the need to warm Redis cache with feature flags
**When** implementing the cache warming service
**Then** create `CacheWarmer` class in `apps/api/lego-api/domains/config/application/cache-warmer.ts`
**And** implement `warmTopFlags(topN: number): Promise<number>` to load top N flags by request volume
**And** use batch Redis operations (pipeline) to minimize network round trips
**And** handle errors gracefully (log failures, don't throw)

**Evidence:**
- `CacheWarmer` class with dependency injection (FlagRepository, RedisCacheAdapter)
- `warmTopFlags()` method implementation
- Unit tests verify batch loading, error handling
- Integration tests against Docker Redis

### AC 2: Batch Redis Operations (Pipeline)
**Given** the need to efficiently load multiple flags into cache
**When** warming cache with 20+ flags
**Then** use Redis pipeline to batch SET operations into single network call
**And** complete all 20 SETs in < 100ms (vs 20 * 50ms = 1 second sequential)
**And** set TTL to 300 seconds (5 minutes) matching WISH-2124 spec

**Evidence:**
- `RedisCacheAdapter.setMany(entries: [key, value, ttl][])` method
- Uses `ioredis` pipeline: `redis.pipeline().set(...).set(...).exec()`
- Performance test: 20 flags cached in < 100ms
- CloudWatch logs show single pipeline execution

### AC 3: Top Flags Selection Strategy
**Given** the need to identify frequently accessed flags
**When** determining which flags to warm
**Then** query `feature_flags` table ordered by `last_accessed_at DESC` (or request count if tracked)
**And** select top 20 flags (configurable via `CACHE_WARMING_TOP_N` env var)
**And** if no access tracking, warm all `enabled=true` flags (fallback strategy)

**Evidence:**
- SQL query: `SELECT * FROM feature_flags WHERE enabled = true ORDER BY last_accessed_at DESC LIMIT 20`
- Unit tests verify query logic, limit parameter
- Integration tests validate correct flags selected
- Environment variable `CACHE_WARMING_TOP_N` configurable

### AC 4: Scheduled Warming Lambda Handler
**Given** the need to warm cache periodically
**When** implementing the scheduled warming handler
**Then** create `warm-cache-handler.ts` Lambda function
**And** trigger cache warming via EventBridge hourly schedule
**And** log warming results: `{ warmed: N, duration: Xms, trigger: "scheduled" }`
**And** complete within 10 seconds (Lambda timeout: 30s)

**Evidence:**
- `apps/api/lego-api/infrastructure/lambdas/warm-cache-handler.ts`
- EventBridge rule configured: `rate(1 hour)`
- CloudWatch logs show hourly executions with results
- Performance: Warming completes in < 10 seconds

### AC 5: Post-Deployment Warming Hook
**Given** deployments flush cache or restart Lambda instances
**When** a new deployment completes
**Then** automatically trigger `warm-cache-handler` before traffic shift
**And** warm cache with top 20 flags
**And** if warming fails, log error but don't block deployment
**And** monitor cache hit rate post-deployment (target: >80% within 1 minute)

**Evidence:**
- CDK/Terraform post-deployment hook configured
- Deployment pipeline logs show warming execution
- Cache hit rate CloudWatch metric shows rapid recovery (>80% within 1 min)
- Failed warming doesn't block deployment (graceful degradation)

### AC 6: Manual Admin Warming Endpoint
**Given** admins need to manually trigger cache warming
**When** implementing the admin warming endpoint
**Then** create `POST /api/admin/cache/warm` endpoint (authenticated, admin role only)
**And** accept optional `flagKeys` array (specific flags to warm)
**And** if no `flagKeys`, warm top 20 flags
**And** return `{ warmed: number, duration: number }` response

**Evidence:**
- Endpoint implementation with authentication/authorization
- Request validation: `flagKeys` is optional string array
- Response matches schema: `{ warmed: number, duration: number }`
- Integration test: Admin user successfully triggers warming
- Non-admin user receives 403 Forbidden

### AC 7: Cold Start Warming Strategy
**Given** Lambda cold starts result in empty local state
**When** a new Lambda instance initializes
**Then** trigger cache warming in background (non-blocking)
**And** if warming fails, log error and continue (lazy population fallback)
**And** warm cache completes within 5 seconds of cold start
**And** first user request benefits from warmed cache (hit rate >90%)

**Evidence:**
- Lambda initialization code triggers warming asynchronously
- CloudWatch logs: "Cold start cache warming initiated"
- Performance test: First request after cold start < 50ms (cache hit)
- Warming failure doesn't block Lambda from serving requests

### AC 8: Cache Warming Metrics and Observability
**Given** the need to monitor cache warming effectiveness
**When** cache warming executes
**Then** emit CloudWatch metrics:
  - `cache_warming_duration` (milliseconds)
  - `cache_warming_flags_count` (number of flags warmed)
  - `cache_warming_errors` (count of failures)
**And** structured logs include: `{ trigger: "scheduled" | "deployment" | "cold-start" | "manual", warmed: N, duration: Xms }`

**Evidence:**
- CloudWatch metrics dashboard shows warming metrics
- Logs queryable: `fields @timestamp, trigger, warmed, duration | filter action = "cache_warming"`
- Alert configured: `cache_warming_errors > 1` triggers PagerDuty/SNS

### AC 9: Error Handling and Graceful Degradation
**Given** cache warming may fail (Redis unavailable, database timeout)
**When** any warming operation encounters an error
**Then** log error with structured logging (`@repo/logger`)
**And** continue execution (don't throw exception)
**And** fall back to lazy population (cache-on-read from WISH-2124)
**And** respond with 200 status (warming failure doesn't fail user requests)

**Evidence:**
- Unit tests mock Redis/database errors, verify graceful handling
- Integration test: Stop Redis, trigger warming, verify no exceptions
- CloudWatch logs show structured error messages
- User requests succeed despite warming failures

### AC 10: Environment-Specific Configuration
**Given** different environments have different warming requirements
**When** configuring cache warming behavior
**Then** support environment variables:
  - `CACHE_WARMING_ENABLED=true` (disable in local dev if needed)
  - `CACHE_WARMING_TOP_N=20` (number of flags to warm)
  - `CACHE_WARMING_SCHEDULE=rate(1 hour)` (EventBridge schedule expression)
**And** local development skips scheduled warming (manual trigger only)

**Evidence:**
- `.env.local`: `CACHE_WARMING_ENABLED=false` (skip scheduled warming)
- `.env.staging`: `CACHE_WARMING_ENABLED=true`, `CACHE_WARMING_TOP_N=10`
- `.env.production`: `CACHE_WARMING_ENABLED=true`, `CACHE_WARMING_TOP_N=20`
- Code checks `CACHE_WARMING_ENABLED` before scheduling

### AC 11: Cache Hit Rate Improvement Validation
**Given** cache warming is implemented
**When** comparing pre-warming vs post-warming cache metrics
**Then** cache hit rate improves from ~80% (lazy) to >95% (warmed)
**And** P95 latency for warmed flags < 50ms (down from 100-200ms)
**And** database query load reduced by 70%+ during warmup periods
**And** post-deployment warmup completes within 1 minute

**Evidence:**
- CloudWatch metrics comparison: Before/after warming implementation
- Load test: 1000 requests, cache hit rate >95%
- Database query logs show 70% reduction in SELECT queries
- Post-deployment metrics show rapid cache recovery (<1 min to >90% hit rate)

### AC 12: Documentation and Runbooks
**Given** operators need to understand and manage cache warming
**When** deploying cache warming to production
**Then** update documentation:
  - `docs/infrastructure/cache-warming.md` - Architecture, configuration, troubleshooting
  - `README.md` - Add cache warming setup instructions
  - Runbook: "Cache warming failures" - diagnosis and mitigation steps

**Evidence:**
- Documentation includes architecture diagrams, config examples
- Runbook covers common issues: Redis connection failures, database timeouts
- README updated with environment variable descriptions

## Reuse Plan

### Packages to Reuse
- **`packages/backend/db`**: Query feature flags from database
- **`@repo/logger`**: Structured logging for warming events and errors
- **`RedisCacheAdapter`**: From WISH-2124, extends with `setMany()` batch operation
- **`FlagRepository`**: From WISH-2009, query top flags by access patterns

### Patterns to Reuse
- **Hexagonal architecture**: `CacheWarmer` in application layer, uses ports (`FlagRepository`, `CacheAdapter`)
- **Dependency injection**: `CacheWarmer` receives dependencies via constructor
- **Error handling**: Graceful degradation pattern from WISH-2124 (log errors, don't throw)
- **Environment configuration**: Same pattern as `REDIS_URL` in `.env` files
- **Lambda handler pattern**: Similar to existing handlers in `apps/api/lego-api/infrastructure/lambdas/`

### Testing Patterns to Reuse
- **Docker Compose integration tests**: Redis + PostgreSQL ephemeral containers
- **MSW for API mocking**: Mock admin warming endpoint for frontend tests
- **CloudWatch metrics validation**: Query metrics after test execution

## Architecture Notes

### Cache Warming Flow

**Scheduled Warming (Hourly):**
```
EventBridge (hourly) → warm-cache-handler
  → CacheWarmer.warmTopFlags(20)
    → FlagRepository.findTopByAccess(20)
    → RedisCacheAdapter.setMany([...flags])
      → Redis pipeline: SET key1, SET key2, ..., SET key20
  → Emit metrics: cache_warming_duration, cache_warming_flags_count
```

**Post-Deployment Warming:**
```
Deployment completes → CDK/Terraform hook
  → warm-cache-handler (triggered once)
    → CacheWarmer.warmTopFlags(20)
      → (same flow as scheduled)
  → Traffic shift to new Lambda instances (cache already warmed)
```

**Cold Start Warming:**
```
Lambda cold start → Initialization phase
  → Background task (async): CacheWarmer.warmTopFlags(20)
  → Lambda ready to serve requests (warming continues in background)
  → First request benefits from warmed cache (if warming completed)
```

**Manual Warming:**
```
Admin → POST /api/admin/cache/warm
  → Authenticate + authorize (admin role)
  → CacheWarmer.warmTopFlags(req.body.flagKeys?.length || 20)
  → Return { warmed: N, duration: Xms }
```

### CacheWarmer Service

```typescript
// apps/api/lego-api/domains/config/application/cache-warmer.ts
import { logger } from '@repo/logger'
import type { FlagRepository } from '../ports/flag-repository'
import type { CacheAdapter } from '../ports/cache-adapter'

export class CacheWarmer {
  constructor(
    private flagRepository: FlagRepository,
    private cacheAdapter: CacheAdapter
  ) {}

  async warmTopFlags(topN: number): Promise<number> {
    const startTime = Date.now()

    try {
      // Query top N flags by access frequency
      const flags = await this.flagRepository.findTopByAccess(topN)

      if (flags.length === 0) {
        logger.warn('No flags found for cache warming')
        return 0
      }

      // Batch cache population using pipeline
      const entries = flags.map(flag => ({
        key: `feature_flags:${process.env.NODE_ENV}:${flag.key}`,
        value: JSON.stringify(flag),
        ttl: 300 // 5 minutes
      }))

      await this.cacheAdapter.setMany(entries)

      const duration = Date.now() - startTime
      logger.info('Cache warming completed', {
        warmed: flags.length,
        duration,
        trigger: 'scheduled'
      })

      return flags.length
    } catch (error) {
      logger.error('Cache warming failed', { error })
      return 0 // Graceful degradation: return 0, don't throw
    }
  }
}
```

### Batch Redis Adapter Extension

```typescript
// apps/api/lego-api/domains/config/adapters/redis-cache-adapter.ts
export class RedisCacheAdapter implements CacheAdapter {
  // ... existing get, set, delete methods from WISH-2124 ...

  async setMany(entries: { key: string; value: string; ttl: number }[]): Promise<void> {
    try {
      const pipeline = this.redis.pipeline()

      for (const { key, value, ttl } of entries) {
        pipeline.setex(key, ttl, value)
      }

      await pipeline.exec()

      logger.debug('Batch cache set completed', { count: entries.length })
    } catch (error) {
      logger.error('Batch cache set failed', { error, count: entries.length })
      // Non-blocking: Batch write failure doesn't throw
    }
  }
}
```

### Lambda Handler

```typescript
// apps/api/lego-api/infrastructure/lambdas/warm-cache-handler.ts
import { logger } from '@repo/logger'
import { createCacheWarmer } from '../../domains/config/di-container'

export const handler = async (event: { triggerType: string }) => {
  logger.info('Cache warming triggered', { trigger: event.triggerType })

  const warmer = createCacheWarmer() // DI container
  const topN = parseInt(process.env.CACHE_WARMING_TOP_N || '20', 10)

  const warmed = await warmer.warmTopFlags(topN)

  return {
    statusCode: 200,
    body: JSON.stringify({
      warmed,
      trigger: event.triggerType
    })
  }
}
```

## Test Plan

See `_pm/TEST-PLAN.md` for comprehensive test plan including:
- Happy path tests (5 tests)
- Error cases (4 tests)
- Edge cases (5 tests)
- Performance tests (cache hit rate improvement, latency reduction)
- Backend testing requirements (.http files, CloudWatch queries)

## UI/UX Notes

**Admin UI Integration (Future):**
- Manual cache warming button in admin dashboard
- Cache metrics visualization (hit rate, warming duration)
- Real-time warming status indicator

**No Frontend Changes Required:**
- This is a backend infrastructure story
- Frontend benefits from improved latency (transparent)

## Dev Feasibility Review

See `_pm/DEV-FEASIBILITY.md` for detailed feasibility analysis including:
- MVP-critical risks (3 risks identified)
- Missing requirements (2 decisions needed)
- Change surface analysis
- MVP evidence expectations
- Confidence: High (builds on proven patterns from WISH-2124)

## Risks & Mitigations

### Risk 1: Warming During High Traffic Periods
**Likelihood**: Medium
**Impact**: Medium (database query spike during warmup)

**Mitigation:**
- Schedule warming during low-traffic hours (e.g., 3 AM UTC)
- Batch database queries (single SELECT with LIMIT 20)
- Monitor database query load, adjust schedule if needed
- Use Redis pipeline to minimize network overhead

### Risk 2: Stale Cache from Infrequent Warming
**Likelihood**: Low
**Impact**: Low (5-minute TTL ensures freshness)

**Mitigation:**
- Hourly warming schedule ensures cache refreshed frequently
- TTL of 5 minutes means stale cache expires quickly
- Cache invalidation on flag updates (from WISH-2124 AC 8)

### Risk 3: Lambda Cold Start Warming Timeout
**Likelihood**: Low
**Impact**: Low (falls back to lazy population)

**Mitigation:**
- Background warming (non-blocking)
- 5-second timeout for warming task
- If warming fails, lazy population works as fallback
- CloudWatch logs track warming failures for diagnosis

### Risk 4: EventBridge Scheduled Trigger Failures
**Likelihood**: Low
**Impact**: Low (hourly retries mitigate)

**Mitigation:**
- EventBridge has built-in retry logic
- Hourly schedule means next warming attempt is only 1 hour away
- CloudWatch alarms for missed warming executions
- Manual warming endpoint available for emergency use

## Deployment Strategy

### Phase 1: Local Development Setup (Week 1)
1. Implement `CacheWarmer` service with unit tests
2. Add `setMany()` batch operation to `RedisCacheAdapter`
3. Create `warm-cache-handler` Lambda function
4. Test manual warming via local HTTP request

### Phase 2: Staging Deployment (Week 1-2)
1. Deploy warming Lambda to staging
2. Configure EventBridge hourly schedule
3. Validate cache hit rate improvement (>95% after warming)
4. Test post-deployment warming hook

### Phase 3: Production Canary (Week 2)
1. Deploy to production with scheduled warming enabled
2. Monitor for 24 hours:
   - Cache hit rate, warming duration, database query load
3. If metrics pass → promote to full production
4. If any metric fails → disable scheduled warming (manual only)

### Phase 4: Post-Deployment Validation (Week 2-3)
1. Monitor CloudWatch metrics for 1 week
2. Validate cache hit rate sustained >95%
3. Verify database query load reduced 70%+
4. Document lessons learned

## Related Stories

**Dependencies:**
- **WISH-2124**: Redis infrastructure setup (provides RedisCacheAdapter)
- **WISH-2009**: Feature flag infrastructure (provides FlagRepository)

**Follow-ups:**
- **WISH-2129**: Real-time cache invalidation via Redis pub/sub (eliminates TTL delay)
- **WISH-2130**: Predictive cache warming using ML (predict flags to warm based on access patterns)
- **WISH-2131**: Multi-region cache warming (global warming for multi-region deployments)

## Definition of Done

- [ ] All 12 acceptance criteria pass
- [ ] Unit tests pass (15+ tests, >80% coverage)
- [ ] Integration tests pass (10+ tests with Docker Redis + PostgreSQL)
- [ ] Performance tests validate cache hit rate >95%, latency <50ms
- [ ] Infrastructure tests pass (EventBridge schedule, Lambda handler)
- [ ] Manual validation complete (warming triggered, metrics visible)
- [ ] Production deployment successful (cache hit rate >95% post-deployment)
- [ ] Documentation updated (README, cache-warming.md, runbook)
- [ ] CloudWatch dashboard shows healthy warming metrics
- [ ] No customer-reported issues for 1 week post-deployment
- [ ] TypeScript compilation passes
- [ ] ESLint passes with no errors
- [ ] Code review approved

## Token Budget

### Phase Summary

| Phase | Estimated | Actual | Delta | Notes |
|-------|-----------|--------|-------|-------|
| Story Generation | ~12k | — | — | Follow-up from WISH-2124 |
| Elaboration | ~10k | — | — | Moderate complexity |
| Implementation | ~12k | — | — | Service + Lambda + Infrastructure |
| Code Review | ~5k | — | — | Standard review |
| **Total** | ~39k | — | — | Medium-sized infrastructure story |

## Agent Log

| Timestamp (America/Denver) | Agent | Action | Outputs |
|---|---|---|---|
| 2026-01-30 00:00 | pm-story-followup-leader | Created follow-up from WISH-2124 finding #2 | WISH-20330.md |

---

## Open Questions

- [ ] Should warming include disabled flags? (Currently: enabled flags only)
- [ ] Should warming support user-specific flags? (Currently: shared flags only)
- [ ] Should warming be region-aware? (Currently: single region)

**Decision Log:**
- All questions deferred to elaboration phase
