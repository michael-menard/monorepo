---
story_id: wrkf-1052
title: elab_graph Observability & Quality
status: backlog
created: 2026-01-27
updated: 2026-01-27
epic: wrkf
feature: LangGraph Orchestrator - Elaboration Observability
depends_on: [wrkf-1051]
split_from: wrkf-1050
split_part: 2 of 4
related_stories:
  - wrkf-1051 # elab_graph MVP (Core Implementation)
---

# wrkf-1052: elab_graph Observability & Quality

## Split Context

This story is part of a split from wrkf-1050.
- **Original Story:** wrkf-1050 (elab_graph Subgraph)
- **Split Reason:** QA Elaboration identified 26 AC (original 8 + 18 additions), exceeding the 8-10 AC threshold. Split into 4 focused stories.
- **This Part:** 2 of 4 (Observability & Quality)
- **Dependency:** Depends on wrkf-1051 (elab_graph MVP - Core Implementation)

**Sibling Stories:**
- wrkf-1051: elab_graph MVP (Core Implementation) - PREREQUISITE
- wrkf-1053: elab_graph Advanced Features
- wrkf-1054: elab_graph Metadata & Linking

## Context

The MVP implementation (wrkf-1051) provides core elaboration generation, but debugging low-quality elaborations, understanding LLM behavior, and improving output quality requires comprehensive observability and quality tooling.

This story adds:
1. **Prompt Observability** - Store and log full prompts sent to Claude for debugging
2. **Error Message Standardization** - Define standard error format (what/why/how)
3. **Dry-Run Mode** - Preview elaborations without writing files (useful for PM validation)
4. **Quality Scoring** - Score elaborations 0-100 based on completeness, coverage, specificity, actionability
5. **Interactive Refinement** - Allow PM feedback and section regeneration for quality improvement

These capabilities enable PMs to:
- Debug why elaborations are low-quality (inspect prompts, context)
- Preview elaborations before committing to filesystem
- Improve quality iteratively via feedback loops
- Monitor quality trends via scoring metrics

## Goal

Add comprehensive observability, quality metrics, and debugging capabilities to the elab_graph subgraph to enable prompt debugging, dry-run previews, standardized error messages, quality scoring, and interactive refinement workflows.

**Success Criteria:**
- PMs can inspect full prompts sent to Claude for debugging
- Error messages follow standard format (what/why/how)
- Dry-run mode allows previewing elaborations without file writes
- Quality scoring provides actionable feedback (0-100 score with breakdown)
- Interactive refinement enables PM-driven quality improvements

## Non-goals

- Automated quality improvement (refinement requires manual PM feedback)
- Quality scoring model training or ML-based scoring (use heuristic scoring)
- Integration with external monitoring tools (Datadog, Grafana) - log-based only
- Visual quality reports or dashboards (CLI output only)
- Prompt template A/B testing (covered in wrkf-1054 - Metadata & Linking)

## Scope

### Packages Affected

**Primary:**
- `packages/backend/orchestrator/src/subgraphs/elab-graph/` (extend existing)
  - `nodes/elaboration-writer.ts` - Add prompt logging
  - `nodes/quality-scorer.ts` - New node for quality scoring
  - `nodes/refinement-coordinator.ts` - New node for interactive refinement
  - `utils/error-formatter.ts` - Standardized error message formatter
  - `graph.ts` - Update graph for dry-run mode and refinement loop

**Supporting:**
- `packages/backend/orchestrator/src/core/state/graph-state.ts` (extend with observability fields)

### Endpoints

**None** - This is backend infrastructure, not HTTP API.

### Data/Storage

**Reads:**
- Existing elaboration files (for refinement)
- PM feedback (via CLI input for refinement)

**Writes:**
- Prompt logs (to GraphState or optional file artifact)
- Quality score reports (to GraphState)

## Acceptance Criteria

### AC1: Prompt Observability
**Given** the elaboration writer node constructs a prompt for Claude API
**When** the prompt is sent to the LLM
**Then** the full prompt (story content + codebase context + template instructions) is stored in GraphState (`observability.prompt`)
**And** the prompt is logged at DEBUG level via `@repo/logger`
**And** the prompt includes metadata: timestamp, story ID, model, max tokens

**Validation:**
- Unit test: GraphState contains full prompt after elaboration writer executes
- Log output includes prompt preview (first 200 chars) at DEBUG level
- GraphState schema includes `observability.prompt` field

### AC2: Error Message Standardization
**Given** any node in elab_graph encounters an error
**When** the error is logged or added to GraphState
**Then** the error message follows standard format:
1. **What failed**: Clear statement (e.g., "Story file not found")
2. **Why it failed**: Root cause (e.g., "File does not exist at path X")
3. **How to fix**: Actionable remedy (e.g., "Create story file at X or check path")

**Validation:**
- Unit test: error formatter produces messages with all 3 components
- Code review: all error paths use standardized formatter
- Example error:
  ```
  What: Story file not found
  Why: File does not exist at /plans/stories/wrkf-1051/wrkf-1051.md
  How: Create the story file at the expected path or verify the story ID is correct
  ```

### AC3: Dry-Run Mode
**Given** the `/elab-story` command is invoked with `--dry-run` flag
**When** the graph executes
**Then** all nodes execute normally (story reader → codebase explorer → elaboration writer → AC validator)
**And** elaboration content is generated and validated
**And** GraphState is populated with all results (elaboration, metrics, validation)
**And** NO file is written to disk
**And** a dry-run summary is logged showing what would have been written

**Validation:**
- Integration test: dry-run mode executes graph without writing file
- Log output shows: "DRY RUN: Would write elaboration to {path} (X bytes)"
- GraphState contains elaboration content for inspection
- Output directory is clean (no elaboration file created)

### AC4: Quality Scoring
**Given** elaboration generation completes successfully
**When** the quality scorer node executes
**Then** a quality score (0-100) is computed based on:
- **Completeness** (25 pts): All required sections present
- **AC Coverage** (25 pts): All ACs addressed in elaboration
- **Specificity** (25 pts): Concrete file paths, types, and code references (not generic)
- **Actionability** (25 pts): Step-by-step implementation guidance (not just descriptions)
**And** the score is added to GraphState (`qualityMetrics.qualityScore`)
**And** a score breakdown is logged (e.g., "Completeness: 25/25, Coverage: 20/25, Specificity: 15/25, Actionability: 20/25 → Total: 80/100")

**Validation:**
- Unit test: quality scorer computes correct score for sample elaboration
- Integration test: GraphState contains quality score after generation
- Low-quality elaboration (missing sections, generic content) scores <50
- High-quality elaboration (complete, specific, actionable) scores >80

**Scoring Heuristics:**
- Completeness: Check for required section headers (Scope, Implementation Plan, AC Breakdown, etc.)
- Coverage: Use AC validator coverage percentage
- Specificity: Count file path references, type names, package names
- Actionability: Detect step-by-step lists, imperative verbs ("Create", "Add", "Update")

### AC5: Interactive Refinement
**Given** elaboration generation completes with quality score <80
**When** PM provides feedback on specific sections or ACs (via CLI prompt)
**Then** the refinement coordinator node re-runs elaboration writer with:
- Original story content + codebase context
- PM feedback appended to prompt (e.g., "AC3 needs more detail on error handling")
- Only the targeted section is regenerated (not full elaboration)
**And** the refined section replaces the original in GraphState
**And** quality scorer re-evaluates the updated elaboration
**And** refinement loop continues until PM approves or max iterations reached (default: 3)

**Validation:**
- Integration test: PM feedback triggers section regeneration
- Mocked LLM returns improved section based on feedback
- Quality score increases after refinement
- Refinement stops after max iterations or PM approval
- GraphState tracks refinement history (iterations, feedback, scores)

**Refinement Loop Flow:**
```
1. Generate elaboration
2. Compute quality score
3. If score < 80 → Prompt PM for feedback
4. PM provides feedback on specific section/AC
5. Regenerate targeted section with feedback
6. Re-score elaboration
7. Repeat steps 3-6 (max 3 iterations) or until PM approves
```

## Reuse Plan

### Existing Components to Reuse

**From wrkf-1051 (elab_graph MVP):**
- Elaboration writer node (extend with prompt logging)
- AC validator (use for quality scoring coverage metric)
- GraphState elaboration fields

**From wrkf-1020 (Node Runner Infrastructure):**
- Node factory pattern for new nodes
- Error handling and logging

**From monorepo:**
- `@repo/logger` for logging

### New Components to Create

**Quality Scorer Node:**
- Heuristic-based scoring (section completeness, coverage, specificity, actionability)
- Score breakdown reporting

**Refinement Coordinator Node:**
- PM feedback collection (CLI prompt)
- Section-specific regeneration
- Refinement loop orchestration

**Error Formatter Utility:**
- Standardized error message formatter (what/why/how)

**Dry-Run Mode:**
- Graph configuration flag to skip file writes

## Architecture Notes

### Observability Fields in GraphState

```typescript
observability: z.object({
  prompt: z.object({
    content: z.string(),
    timestamp: z.string(),
    storyId: z.string(),
    model: z.string(),
    maxTokens: z.number(),
  }),
  dryRun: z.boolean(),
  refinementHistory: z.array(z.object({
    iteration: z.number(),
    feedback: z.string(),
    scoreBefore: z.number(),
    scoreAfter: z.number(),
  })),
}).optional(),

qualityMetrics: z.object({
  // ... existing fields from wrkf-1051 ...
  qualityScore: z.number(), // 0-100
  scoreBreakdown: z.object({
    completeness: z.number(), // 0-25
    coverage: z.number(), // 0-25
    specificity: z.number(), // 0-25
    actionability: z.number(), // 0-25
  }),
}).optional(),
```

## Test Plan

### Happy Path Tests

#### Test 1: Prompt Logging
**Setup:** Story content and codebase context in GraphState
**Action:** Execute elaboration writer node
**Expected:** GraphState contains full prompt in `observability.prompt`
**Evidence:** GraphState inspection, log shows prompt preview

#### Test 2: Standardized Error Messages
**Setup:** Story file does not exist
**Action:** Execute story reader node
**Expected:** Error message includes what/why/how components
**Evidence:** Log output matches format, error is actionable

#### Test 3: Dry-Run Mode
**Setup:** Valid story, invoke with `--dry-run` flag
**Action:** Execute graph in dry-run mode
**Expected:** Elaboration generated, validated, but not written to disk
**Evidence:** Log shows "DRY RUN: Would write...", no file created

#### Test 4: Quality Scoring
**Setup:** Elaboration generated with all sections
**Action:** Execute quality scorer node
**Expected:** Score computed (0-100) with breakdown
**Evidence:** GraphState contains quality score and breakdown, log shows score

#### Test 5: Interactive Refinement
**Setup:** Elaboration with quality score 65
**Action:** PM provides feedback "AC3 needs error handling details"
**Expected:** Refinement loop regenerates AC3 section, re-scores, quality improves
**Evidence:** GraphState shows refinement history, score increases

### Error Cases

#### Error 1: Dry-Run with File Write Attempt
**Setup:** Dry-run mode enabled
**Action:** Attempt to write elaboration file
**Expected:** Write is skipped, no error thrown
**Evidence:** Log confirms dry-run skip, no file created

#### Error 2: Quality Scorer with Incomplete Elaboration
**Setup:** Elaboration missing required sections
**Action:** Execute quality scorer
**Expected:** Low completeness score (<15/25)
**Evidence:** Score breakdown shows low completeness

#### Error 3: Refinement Max Iterations Exceeded
**Setup:** PM provides feedback for 4 iterations
**Action:** Refinement loop executes
**Expected:** Loop stops after 3 iterations with warning
**Evidence:** Log shows "Max refinement iterations (3) reached"

## UI/UX Notes

**Not applicable** - This is backend infrastructure with CLI interactions only.

---

## Implementation Checklist (For Dev)

Before creating PR:
- [ ] Prompt logging tested (GraphState contains full prompt)
- [ ] Error formatter tested (all error paths use standard format)
- [ ] Dry-run mode tested (no file writes in dry-run)
- [ ] Quality scorer tested (score 0-100 with breakdown)
- [ ] Interactive refinement tested (section regeneration works)
- [ ] All unit tests pass
- [ ] Integration test with dry-run mode passes
- [ ] Refinement loop tested with max iterations
- [ ] Code review confirms error standardization applied to all nodes
