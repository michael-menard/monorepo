---
story_id: wrkf-1042
title: pm_generate_graph Subgraph - Advanced Features
status: backlog
split_from: wrkf-1040
split_part: 2 of 2
created: 2026-01-27
updated: 2026-01-27
epic: wrkf
feature: LangGraph Orchestrator - Story Generation Workflow
depends_on:
  - wrkf-1041 # Core pm_generate_graph infrastructure must be complete first
related_stories:
  - wrkf-1010 # GraphState Schema
  - wrkf-1020 # Node Runner Infrastructure
  - wrkf-1140 # Evidence Bundle (for evidence integration)
---

# WRKF-1042: pm_generate_graph Subgraph - Advanced Features

## Split Context

This story is part of a split from WRKF-1040.
- **Original Story:** WRKF-1040 (pm_generate_graph Subgraph)
- **Split Reason:** Elaboration identified 15 additional gaps and enhancements (from 8 to 23 ACs), exceeding sustainable scope for single story
- **This Part:** 2 of 2 (Advanced features and enhancements)
- **Dependency:** Depends on WRKF-1041 (core pm_generate_graph infrastructure)
- **Predecessor:** WRKF-1041 must be completed first

This split focuses on advanced features that enhance the pm_generate_graph subgraph: batch generation, quality scoring, interactive refinement, context window overflow handling, and evidence integration.

## Context

With the core pm_generate_graph subgraph implemented in WRKF-1041, this story adds advanced features to improve story generation quality, efficiency, and integration with the broader orchestrator ecosystem. These enhancements address edge cases (context window overflow), improve developer experience (batch generation), and add quality safeguards (quality scoring agent).

The advanced features include:
1. Batch story generation to process multiple stories in parallel (respecting LLM rate limits)
2. Quality scoring agent to evaluate generated stories beyond structural validation
3. Interactive refinement loop to enable multi-turn story improvement
4. Context window overflow handling to gracefully manage large plan documents
5. Evidence bundle integration to include evidence from completed stories in generation context

These features build on the solid foundation established in WRKF-1041 and integrate with other orchestrator components (evidence bundle from wrkf-1140).

## Goal

Extend the pm_generate_graph subgraph with advanced features that improve story generation quality, support batch processing, enable iterative refinement, and integrate with the orchestrator evidence system.

**Success Criteria:**
- Batch generation processes multiple stories in parallel without exceeding LLM rate limits
- Quality scoring agent identifies common story quality issues (vague ACs, missing risks, incomplete test plans)
- Interactive refinement loop enables PM to provide feedback and regenerate improved stories
- Context window overflow is detected and handled gracefully (truncate with warning or fail with transparency)
- Evidence from completed stories is included in generation context to maintain consistency
- All advanced features integrate cleanly with core pm_generate_graph infrastructure from WRKF-1041

## Non-goals

- Template marketplace with epic-specific templates (defer to future enhancement after multi-epic patterns emerge)
- Semantic search for similar stories (requires vector DB integration - high effort)
- Plan document summarization node (adds LLM complexity - consider for v2)
- Story diff preview mode (UX enhancement for regeneration - out of scope)
- Caching plan excerpts (low ROI for v1 - valuable in batch scenario, but defer)
- Validation rule configuration file (premature without multi-epic usage patterns)
- LLM response streaming for UX (CLI feedback polish - defer to future iteration)
- Automatic story approval without PM review (manual PM review remains final gate)

## Scope

### Packages Affected

**Primary:**
- `packages/orchestrator/src/subgraphs/pm-generate-graph/` (extend existing from WRKF-1041)
  - `nodes/batch-orchestrator.ts` - Node to manage parallel story generation
  - `nodes/quality-scorer.ts` - Node to score story quality
  - `nodes/refinement-coordinator.ts` - Node to handle multi-turn refinement
  - `nodes/context-overflow-handler.ts` - Node to detect and handle context overflow
  - `nodes/evidence-integrator.ts` - Node to include evidence bundle in context
  - `__tests__/` - Unit and integration tests for new nodes

**Supporting:**
- `packages/orchestrator/src/core/state/graph-state.ts` (extend with batch and refinement fields)
- `packages/orchestrator/src/utils/rate-limiter.ts` (new file - LLM rate limiting)
- `packages/orchestrator/src/utils/quality-scoring-rules.ts` (new file - scoring criteria)

**Adjacent:**
- `.claude/commands/pm-generate-story.md` - Update documentation with batch and refinement flags

### Endpoints

**None** - This is backend infrastructure, not HTTP API.

New CLI flags for `/pm-generate-story` command:
- `--batch` - Enable batch generation mode (requires multiple story IDs)
- `--refine` - Enable interactive refinement mode
- `--include-evidence` - Include evidence bundle in context (requires WRKF-1140)

### Data/Storage

**Reads:**
- Story index file (same as WRKF-1041)
- Plan documents (same as WRKF-1041)
- Evidence bundle files (e.g., `EVIDENCE.md` from completed stories) - requires WRKF-1140

**Writes:**
- Story documents (same as WRKF-1041)
- Quality score metadata (optional - can be logged or stored in story frontmatter)

**No database access** - all I/O is filesystem-based.

## Acceptance Criteria

### AC1: Batch Story Generation with Rate Limiting
**Given** multiple story IDs are provided to pm_generate_graph with `--batch` flag
**When** the batch orchestrator node executes
**Then** stories are generated in parallel with concurrency limit (default: 3 concurrent LLM calls)
**And** LLM rate limits are respected (max requests per minute based on Anthropic tier)
**And** if a single story fails, other stories continue processing (fail-partial, not fail-all)
**And** batch results are aggregated: successful stories, failed stories, and error details
**And** GraphState contains `batchResults` with per-story success/failure status

**Validation:**
- Integration test verifies parallel story generation respects concurrency limit
- Integration test verifies rate limiting prevents exceeding Anthropic limits
- Unit test verifies partial failure handling (one story fails, others succeed)
- Manual test with real LLM confirms batch generation completes without rate limit errors

**Rate Limiting:**
- Anthropic tier detection via API key (if available) or environment variable
- Default: 50 requests/minute (Standard tier)
- Backoff strategy: if rate limited, wait and retry batch

### AC2: Quality Scoring Agent Evaluates Generated Stories
**Given** a story is generated by the story writer node
**When** the quality scorer node executes (after validation, before file write)
**Then** the story is evaluated against quality criteria:
  - **AC Concreteness**: Are acceptance criteria specific and testable? (no "should be fast", "should work well")
  - **Test Plan Completeness**: Does the test plan cover all ACs? Are assertions defined?
  - **Risk Disclosure**: Are risks explicitly listed with mitigations?
  - **Reuse Plan**: Are existing components identified for reuse?
**And** a quality score (0-100) is calculated based on criteria pass/fail
**And** quality findings (warnings/suggestions) are added to GraphState
**And** if quality score is below threshold (e.g., 60), a warning is logged but story is NOT blocked

**Validation:**
- Unit test verifies quality scoring logic for each criterion
- Unit test verifies score calculation (all pass = 100, partial fail = proportional)
- Integration test confirms quality findings are added to GraphState
- Manual test reviews quality scoring on real generated stories

**Scoring Criteria:**
- AC Concreteness: 25 points
- Test Plan Completeness: 25 points
- Risk Disclosure: 25 points
- Reuse Plan: 25 points

### AC3: Interactive Refinement Loop for Story Improvement
**Given** a story is generated and quality scored
**When** the `--refine` flag is present
**Then** the refinement coordinator node prompts the PM for feedback (CLI prompt or interactive UI)
**And** PM can provide feedback text (e.g., "Make ACs more specific", "Add risks for LLM hallucinations")
**And** the story writer node is re-invoked with feedback appended to prompt
**And** the refinement loop repeats up to 3 times or until PM approves
**And** GraphState contains `refinementHistory` with feedback and iteration count

**Validation:**
- Integration test with mocked PM feedback confirms refinement loop executes
- Unit test verifies feedback is appended to LLM prompt correctly
- Manual test with real PM feedback validates refinement improves story quality

**Refinement Workflow:**
1. Generate initial story
2. Quality score story
3. Prompt PM for feedback (if --refine flag)
4. If feedback provided, regenerate story with feedback context
5. Repeat up to 3 iterations or until PM approves
6. Write final story to disk

### AC4: Context Window Overflow Detection and Handling
**Given** plan documents and index entry are large (combined context >100KB)
**When** the context gatherer node executes
**Then** the total context size is measured (in bytes and estimated tokens)
**And** if context exceeds threshold (e.g., 100KB or ~25,000 tokens), overflow handling is triggered
**And** overflow handling behavior is configurable:
  - **Fail with transparency**: Return error with context size details and suggested fixes
  - **Truncate with warning**: Prioritize critical sections (Architecture Notes, Reuse Plan), truncate less critical sections, log warning
**And** GraphState contains `contextOverflow` flag and overflow details (size, threshold, action taken)

**Validation:**
- Unit test verifies context size measurement (bytes and tokens)
- Unit test verifies overflow detection triggers when threshold exceeded
- Integration test confirms "fail with transparency" behavior (error returned with details)
- Integration test confirms "truncate with warning" behavior (context truncated, warning logged)

**Default Behavior:** Fail with transparency (safer for v1 - prevents silent data loss)

### AC5: Evidence Bundle Integration for Context Enrichment
**Given** WRKF-1140 (Evidence Bundle) is completed
**And** the `--include-evidence` flag is present
**When** the evidence integrator node executes (after context gatherer, before story writer)
**Then** evidence bundle files from related completed stories are read
**And** relevant evidence excerpts are added to story generation context:
  - Implementation patterns used
  - Test strategies employed
  - Architecture decisions made
**And** evidence excerpts are limited to 10KB total (in addition to 20KB plan excerpts from AC2 of WRKF-1041)
**And** GraphState contains `evidenceContext` with evidence excerpts

**Validation:**
- Integration test verifies evidence bundle files are read correctly
- Unit test verifies evidence excerpt extraction logic
- Integration test confirms evidence context is included in LLM prompt
- Manual test validates evidence improves story consistency with completed stories

**Evidence Prioritization:**
- Prioritize evidence from stories in `related_stories` field
- Extract "Architecture Notes", "Reuse Plan", and "Implementation Patterns" sections

### AC6: GraphState Extensions for Advanced Features
**Given** the GraphState extensions from WRKF-1041
**When** this story extends GraphState further
**Then** the following fields are added (Zod schema):
- `batchResults`: array of objects with `storyId: string`, `status: 'success' | 'failed'`, `error?: string`
- `qualityScore`: object with `score: number`, `findings: string[]`
- `refinementHistory`: array of objects with `iteration: number`, `feedback: string`, `timestamp: string`
- `contextOverflow`: object with `detected: boolean`, `size: number`, `threshold: number`, `action: 'fail' | 'truncate'`
- `evidenceContext`: string (evidence excerpts)

**Validation:**
- Unit test verifies Zod schema validation for new fields
- Type-checking passes (no TypeScript errors on GraphState usage)

## Reuse Plan

### Existing Components to Reuse

**From WRKF-1041 (Core pm_generate_graph):**
- All nodes: template loader, context gatherer, story writer, validator
- GraphState extensions for story generation
- Utility modules: file-io, markdown-parser, yaml-frontmatter
- Claude API client wrapper
- Story writer prompt template

**From wrkf-1140 (Evidence Bundle):**
- Evidence bundle schema and reader utilities (once wrkf-1140 is complete)

**From wrkf-1020 (Node Runner Infrastructure):**
- Node factory pattern
- Error handling and retry logic
- Logging integration

**From monorepo:**
- `@repo/logger` for logging
- Zod for schema validation

### New Components to Create

**Rate Limiter:**
- `packages/orchestrator/src/utils/rate-limiter.ts` - LLM rate limiting with configurable limits

**Quality Scoring Rules:**
- `packages/orchestrator/src/utils/quality-scoring-rules.ts` - Quality criteria definitions and scoring logic

**Context Overflow Handler:**
- `packages/orchestrator/src/subgraphs/pm-generate-graph/nodes/context-overflow-handler.ts` - Context size measurement and overflow handling

**Batch Orchestrator:**
- `packages/orchestrator/src/subgraphs/pm-generate-graph/nodes/batch-orchestrator.ts` - Parallel story generation coordinator

## Architecture Notes

### Ports & Adapters Alignment

**Ports (Interfaces):**
- **Batch Generation Port**: Interface for parallel story generation
- **Quality Scoring Port**: Interface for story quality evaluation
- **Refinement Port**: Interface for interactive refinement loop

**Adapters (Implementations):**
- **Rate Limiter Adapter**: Anthropic tier detection and rate limit enforcement
- **Evidence Reader Adapter**: Evidence bundle filesystem reader (depends on wrkf-1140)

**Domain Logic (Core):**
- Quality scoring criteria and rules
- Context overflow detection and truncation logic
- Batch orchestration and partial failure handling
- Refinement loop iteration limits

**Flow:**
```
Batch Mode:
CLI Command → Batch Orchestrator → [Story 1, Story 2, Story 3] in parallel → Rate Limiter → Core pm_generate_graph nodes → Aggregated Results

Refinement Mode:
Core pm_generate_graph → Quality Scorer → Refinement Coordinator → PM Feedback → Story Writer (re-invoke) → Repeat up to 3x → Final Story
```

### Node Execution Flow Extensions

**Batch Mode:**
```
START
  ↓
Batch Orchestrator Node
  - Parse multiple story IDs
  - Create parallel execution plan
  - Enforce concurrency limit (3 concurrent)
  ↓
For each story (in parallel):
  - Execute core pm_generate_graph nodes (from WRKF-1041)
  - Handle individual failures without blocking others
  ↓
Aggregate Results
  - Collect success/failure status per story
  - Log batch summary
  ↓
END (with batch results)
```

**Refinement Mode:**
```
Core pm_generate_graph completes
  ↓
Quality Scorer Node
  - Evaluate story against quality criteria
  - Calculate quality score
  - Add findings to GraphState
  ↓
Refinement Coordinator Node (if --refine flag)
  - Prompt PM for feedback (CLI or UI)
  - If feedback provided, append to LLM prompt
  - Re-invoke Story Writer node
  - Increment iteration count
  - Repeat up to 3 times or until PM approves
  ↓
File Writer (write final refined story)
  ↓
END
```

**Evidence Integration:**
```
Context Gatherer Node (from WRKF-1041)
  ↓
Evidence Integrator Node (if --include-evidence flag)
  - Read evidence bundle files from related stories
  - Extract relevant evidence excerpts (Architecture Notes, Reuse Plan, Implementation Patterns)
  - Add evidence to GraphState (max 10KB)
  ↓
Story Writer Node (context includes evidence)
  ↓
Continue with core flow
```

## Infrastructure Notes

### Dependencies

**Same as WRKF-1041:**
- LangGraphJS
- Anthropic SDK
- Zod
- `@repo/logger`

**New dependencies:**
- None (all features implemented with existing dependencies)

### Environment Variables

**Same as WRKF-1041:**
- `ANTHROPIC_API_KEY`
- `STORY_TEMPLATE_PATH` (optional)
- `LOG_LEVEL` (optional)

**New optional variables:**
- `ANTHROPIC_TIER` - Override Anthropic tier for rate limiting (default: auto-detect or Standard)
- `BATCH_CONCURRENCY_LIMIT` - Override default concurrency limit for batch generation (default: 3)

### Runtime Considerations

**Performance:**
- Batch generation: parallelism reduces total time, but concurrency limit prevents rate limit errors
- Quality scoring: adds ~1-2 seconds per story (minimal overhead)
- Refinement loop: each iteration adds ~10-30 seconds (LLM call), up to 3 iterations

**Concurrency:**
- Batch generation uses concurrency limit to avoid overwhelming LLM API
- Rate limiter prevents exceeding Anthropic tier limits

**Error Recovery:**
- Batch mode: partial failures handled gracefully (continue processing other stories)
- Refinement mode: if LLM fails during refinement, fall back to previous iteration

## HTTP Contract Plan

**Not applicable** - This is backend infrastructure, not an HTTP API.

## Seed Requirements

**Not applicable** - No database seeding required.

## Test Plan

### Scope Summary

**Endpoints touched:** None (infrastructure code)
**UI touched:** No (CLI flags only)
**Data/storage touched:** No (filesystem I/O only)

**Components affected:**
- `packages/orchestrator/src/subgraphs/pm-generate-graph/` (new nodes)
- GraphState extensions for batch, quality, refinement, overflow, evidence
- Rate limiting and quality scoring utilities

### Happy Path Tests

#### Test 1: Batch Generation Processes Multiple Stories
**Setup:**
- 5 story IDs provided with `--batch` flag
- All stories have valid index entries

**Action:**
- Invoke pm_generate_graph with --batch flag

**Expected outcome:**
- All 5 stories generated successfully
- Stories processed in parallel (concurrency limit: 3)
- No rate limit errors
- Batch results show 5 successes, 0 failures

**Evidence:**
- All 5 story files exist at expected paths
- Logs show parallel execution
- GraphState.batchResults shows 5 successes

#### Test 2: Quality Scoring Evaluates Story
**Setup:**
- Story generated with all quality criteria met

**Action:**
- Execute quality scorer node

**Expected outcome:**
- Quality score = 100 (all criteria passed)
- No quality findings/warnings
- Story is written to disk

**Evidence:**
- GraphState.qualityScore.score = 100
- GraphState.qualityScore.findings = []

#### Test 3: Interactive Refinement Improves Story
**Setup:**
- Initial story generated with vague ACs
- PM provides feedback: "Make ACs more specific"

**Action:**
- Invoke pm_generate_graph with --refine flag
- Provide feedback when prompted

**Expected outcome:**
- Story writer re-invoked with feedback context
- Refined story has more specific ACs
- Refinement iteration count = 2 (initial + 1 refinement)

**Evidence:**
- GraphState.refinementHistory contains feedback entry
- Refined story file shows improved ACs
- Logs show 2 LLM calls (initial + refinement)

#### Test 4: Evidence Integration Includes Completed Story Context
**Setup:**
- WRKF-1140 completed, evidence bundle available for related story
- `--include-evidence` flag present

**Action:**
- Generate story with evidence integration

**Expected outcome:**
- Evidence excerpts added to context
- Story writer prompt includes evidence
- Generated story references patterns from evidence

**Evidence:**
- GraphState.evidenceContext contains evidence excerpts
- Logs show evidence bundle files read
- Generated story content shows consistency with evidence

### Error Cases

#### Error 1: Batch Generation with Partial Failures
**Setup:** 3 story IDs, one has invalid index entry
**Action:** Invoke batch generation
**Expected:** 2 stories succeed, 1 fails, batch continues processing
**Evidence:** GraphState.batchResults shows 2 successes, 1 failure with error details

#### Error 2: Context Window Overflow (Fail Mode)
**Setup:** Plan documents total 150KB, threshold = 100KB, overflow mode = fail
**Action:** Execute context gatherer
**Expected:** Graph fails with error "Context size exceeds threshold (150KB > 100KB)"
**Evidence:** Error logged with context size details, no LLM call

#### Error 3: Context Window Overflow (Truncate Mode)
**Setup:** Plan documents total 150KB, threshold = 100KB, overflow mode = truncate
**Action:** Execute context gatherer
**Expected:** Context truncated to 100KB, warning logged, LLM call proceeds
**Evidence:** Warning logged with truncation details, LLM call succeeds

### Edge Cases

#### Edge 1: Refinement Loop Reaches Iteration Limit
**Setup:** PM provides feedback for 3 refinement iterations
**Action:** Execute refinement loop
**Expected:** Loop terminates after 3 iterations, final story written
**Evidence:** GraphState.refinementHistory has 3 entries, final story written

#### Edge 2: Quality Score Below Threshold
**Setup:** Generated story has vague ACs, quality score = 50
**Action:** Execute quality scorer
**Expected:** Warning logged, story written (not blocked)
**Evidence:** Log shows quality warning, story file exists

### Required Tooling Evidence

**Backend (LangGraph Orchestrator):**

**Required test execution:**
- Unit tests for each new node (batch orchestrator, quality scorer, refinement coordinator, context overflow handler, evidence integrator)
- Integration test for batch generation
- Integration test for refinement loop
- Integration test for evidence integration

**Assertions:**
- Batch generation respects concurrency limit and rate limits
- Quality scoring evaluates all criteria correctly
- Refinement loop iterates correctly with feedback
- Context overflow triggers appropriate behavior (fail or truncate)
- Evidence integration includes evidence in LLM context

**Test framework:**
- Vitest for unit and integration tests
- Mocked LLM client for deterministic tests
- Mocked evidence bundle reader for evidence integration tests

**Evidence artifacts:**
- Test coverage report (>45% minimum, aim for >80% on new code)
- Sample batch results from test runs
- Quality scoring output examples
- Refinement loop iteration logs

**Frontend:** Not applicable - backend infrastructure only

### Risks to Call Out

**Risk 1: Rate Limiting Complexity**
- **Description:** Anthropic tier detection may fail, causing incorrect rate limits
- **Mitigation:** Provide manual override via environment variable, log tier detection, use conservative defaults

**Risk 2: Refinement Loop Abuse**
- **Description:** PM may iterate excessively, wasting LLM tokens
- **Mitigation:** Hard limit of 3 iterations, log token usage per iteration

**Risk 3: Quality Scoring False Positives**
- **Description:** Quality scoring may incorrectly flag high-quality stories
- **Mitigation:** Use warnings (not blockers), allow PM override, iterate scoring rules based on feedback

**Risk 4: Evidence Integration Dependency**
- **Description:** Depends on wrkf-1140 completion, may block this story
- **Mitigation:** Make evidence integration optional (flag-gated), story can complete without wrkf-1140

**Risk 5: Context Overflow Edge Cases**
- **Description:** Truncation logic may remove critical context
- **Mitigation:** Prioritize critical sections (Architecture Notes, Reuse Plan), fail with transparency as default, log truncation details

## UI/UX Notes

**Not applicable** - This story implements backend infrastructure (LangGraph subgraph) with CLI flags only.

**CLI UX Enhancements:**
- Batch generation: Show progress indicator (e.g., "Processing story 3 of 5...")
- Refinement loop: Clear prompts for PM feedback with iteration count
- Quality scoring: Display quality score and findings in CLI output

---

## Implementation Checklist (For Dev)

Before creating PR:
- [ ] All unit tests pass locally (`pnpm test`)
- [ ] Integration tests pass locally (batch, refinement, evidence integration)
- [ ] Lint and type-check pass (`pnpm lint`, `pnpm check-types`)
- [ ] Batch generation tested with multiple stories
- [ ] Quality scoring tested with various story quality levels
- [ ] Refinement loop tested with PM feedback
- [ ] Context overflow tested (fail and truncate modes)
- [ ] Evidence integration tested (requires wrkf-1140 or mocked evidence)
- [ ] CLI flags documented in command documentation
- [ ] Rate limiting tested with mocked LLM responses
- [ ] GraphState extensions validated with Zod
- [ ] Error messages are actionable
