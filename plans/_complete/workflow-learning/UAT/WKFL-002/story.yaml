id: WKFL-002
title: "Confidence Calibration"
status: uat
priority: P1
phase: qa
created_at: 2026-02-06T17:00:00-07:00

epic: workflow-learning
prefix: WKFL

dependencies:
  - WKFL-001
  - WKFL-004
blocks:
  - WKFL-003
  - WKFL-010

owner: null
estimated_tokens: 50000

tags:
  - analysis
  - calibration
  - kb-integration

summary: |
  Track stated confidence vs actual outcomes per agent, compute calibration
  scores, and generate threshold adjustment recommendations.

goal: |
  Build a calibration system that:
  1. Tracks when agents said "high confidence" and were wrong
  2. Computes accuracy per agent per confidence level
  3. Alerts when accuracy drops below thresholds
  4. Generates specific threshold adjustment recommendations

non_goals:
  - Auto-adjusting thresholds (proposals only)
  - Real-time calibration updates
  - Cross-project calibration

scope:
  in:
    - Calibration tracking schema in KB
    - confidence-calibrator.agent.md (haiku)
    - Weekly calibration job
    - /calibration-report command
    - CALIBRATION-{date}.yaml output
    - Threshold adjustment recommendations

  out:
    - Auto-applying threshold changes (WKFL-003)
    - Pattern mining (WKFL-006)

acceptance_criteria:
  - id: AC-1
    description: "Track: agent, finding, stated confidence, actual outcome"
    verification: "Query KB for calibration entries, verify all fields populated"

  - id: AC-2
    description: "Compute accuracy per agent per confidence level"
    verification: "CALIBRATION-{date}.yaml has accuracy scores by agent by level"

  - id: AC-3
    description: "Alert when 'high' accuracy drops below 90%"
    verification: "Report flags agents below threshold"

  - id: AC-4
    description: "Generate threshold adjustment recommendations"
    verification: "Recommendations are specific (e.g., 'raise high threshold from 0.85 to 0.90')"

  - id: AC-5
    description: "Weekly job runs and produces report"
    verification: "/calibration-report produces CALIBRATION-{date}.yaml"

technical_notes: |
  ## Calibration Entry Schema

  ```yaml
  type: calibration_point
  agent_id: code-review-security
  finding_id: SEC-042
  story_id: WISH-2045
  stated_confidence: high
  actual_outcome: false_positive  # from WKFL-004 feedback
  timestamp: 2026-02-06T15:30:00Z
  ```

  ## Accuracy Calculation

  For each (agent, confidence_level):
    accuracy = correct_predictions / total_predictions

  Where correct means:
  - Finding NOT marked as false_positive
  - Finding NOT marked as severity_wrong

  ## Threshold Recommendation Logic

  If agent's "high" accuracy < 0.90:
    Recommend: "Tighten high threshold from X to X+0.05"

  If agent's "low" accuracy > 0.85:
    Recommend: "Trust low confidence more, consider promoting to medium"

reuse_plan:
  must_reuse:
    - KB tools
    - Feedback data from WKFL-004
    - Outcome data from WKFL-001

  may_create:
    - confidence-calibrator.agent.md
    - /calibration-report command
    - Calibration schemas

token_budget:
  estimated: 50000
  enforcement: warning
