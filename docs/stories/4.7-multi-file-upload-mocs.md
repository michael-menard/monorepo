# Story 4.7: Implement Multi-File Upload for MOCs

**Epic**: 4 - User Profile & Advanced Features Migration

**As a** user,
**I want** to upload multiple instruction files at once,
**so that** I can efficiently add complete documentation sets.

## Acceptance Criteria

1. Lambda handler enhanced to support multiple files in `POST /api/mocs/{id}/files`
2. Multipart parsing accepts up to 10 files per request
3. Each file validated independently (type, size per `@monorepo/file-validator`)
4. Files uploaded to S3 in parallel using `Promise.all()`
5. Database records inserted in batch transaction to `mocFiles` table
6. Partial success handling: if some uploads fail, successful ones are recorded, errors returned for failed ones
7. Lambda timeout: 120 seconds, memory: 2048 MB
8. Response: `{ success: true, data: { uploaded: [...], failed: [...] } }`
9. Error details include file name and reason for failure
10. Total payload size limited to 50 MB

## Implementation Status

**Status**: Not Started

## Files Modified

_To be populated during implementation_

## QA Results

_Pending implementation and review_

---

## Requirements Traceability Matrix

| AC # | Requirement                            | Test Coverage | Status   |
| ---- | -------------------------------------- | ------------- | -------- |
| 1    | Multi-file support in POST endpoint    | TBD           | ⏳ PENDING |
| 2    | Accept up to 10 files                  | TBD           | ⏳ PENDING |
| 3    | Independent file validation            | TBD           | ⏳ PENDING |
| 4    | Parallel S3 upload                     | TBD           | ⏳ PENDING |
| 5    | Batch database transaction             | TBD           | ⏳ PENDING |
| 6    | Partial success handling               | TBD           | ⏳ PENDING |
| 7    | Lambda timeout 120s, memory 2048 MB    | TBD           | ⏳ PENDING |
| 8    | Response format (uploaded + failed)    | TBD           | ⏳ PENDING |
| 9    | Error details per file                 | TBD           | ⏳ PENDING |
| 10   | 50 MB total payload limit              | TBD           | ⏳ PENDING |

**Overall Coverage: TBD**

---

## Test Summary

_To be populated during implementation_

### Recommended Test Coverage

1. **Successful Uploads** - 3 tests
   - Upload 3 valid files successfully
   - Verify all stored in S3
   - Verify all database records created

2. **Validation** - 5 tests
   - 11 files rejected (exceeds limit)
   - Invalid file type in batch → partial success
   - File too large in batch → partial success
   - Total payload >50MB → 400 error
   - Empty files array → 400 error

3. **Partial Success** - 3 tests
   - 2 valid + 1 invalid → 2 uploaded, 1 failed
   - S3 failure for one file → others succeed
   - Verify database only has successful uploads

4. **Authorization** - 2 tests
   - User can only upload to own MOC
   - 403 when uploading to another user's MOC

5. **Performance** - 2 tests
   - 10 files upload in parallel (<60 seconds)
   - Transaction rollback on critical error

---

## Technical Notes

### Multipart Parsing for Multiple Files

```typescript
import { parseMultipartForm } from '@/lib/utils/multipart-parser'

// Parse multipart form data with multiple files
const formData = await parseMultipartForm(event)
const files = formData.filter(part => part.type === 'file')

// Validate file count
if (files.length === 0) {
  return createErrorResponse(400, 'BAD_REQUEST', 'No files provided')
}

if (files.length > 10) {
  return createErrorResponse(400, 'VALIDATION_ERROR', 'Maximum 10 files per upload')
}

// Validate total payload size
const totalSize = files.reduce((sum, file) => sum + file.buffer.length, 0)
if (totalSize > 50 * 1024 * 1024) { // 50MB
  return createErrorResponse(400, 'VALIDATION_ERROR', 'Total payload exceeds 50MB limit')
}
```

### Individual File Validation

```typescript
import { validateFile, createFileValidationConfig } from '@monorepo/file-validator'

interface FileValidationResult {
  file: FormFile
  isValid: boolean
  errors?: string[]
}

function validateFiles(files: FormFile[]): FileValidationResult[] {
  return files.map(file => {
    const result = validateFile(
      {
        fieldname: 'files',
        originalname: file.filename,
        encoding: file.encoding || '7bit',
        mimetype: file.mimetype,
        size: file.buffer.length,
      },
      createFileValidationConfig(10 * 1024 * 1024) // 10MB per file
    )

    return {
      file,
      isValid: result.isValid,
      errors: result.errors,
    }
  })
}
```

### Parallel S3 Upload

```typescript
interface UploadResult {
  filename: string
  success: boolean
  s3Url?: string
  error?: string
}

async function uploadFilesParallel(
  files: FormFile[],
  mocId: string,
  userId: string
): Promise<UploadResult[]> {
  const uploadPromises = files.map(async (file) => {
    try {
      // Validate file
      const validationResult = validateFile(file, config)
      if (!validationResult.isValid) {
        return {
          filename: file.filename,
          success: false,
          error: validationResult.errors?.join(', ') || 'Validation failed',
        }
      }

      // Generate unique key
      const fileId = uuidv4()
      const ext = file.filename.split('.').pop()
      const s3Key = `mocs/${userId}/${mocId}/files/${fileId}.${ext}`

      // Upload to S3
      const s3Url = await uploadToS3({
        key: s3Key,
        body: file.buffer,
        contentType: file.mimetype,
      })

      return {
        filename: file.filename,
        success: true,
        s3Url,
        fileId,
      }
    } catch (error) {
      return {
        filename: file.filename,
        success: false,
        error: error.message,
      }
    }
  })

  // Execute all uploads in parallel
  return Promise.all(uploadPromises)
}
```

### Batch Database Transaction

```typescript
import { db } from '@/lib/db/client'
import { mocFiles } from '@/db/schema'

// Only insert successful uploads
const successfulUploads = uploadResults.filter(r => r.success)

if (successfulUploads.length > 0) {
  await db.transaction(async (tx) => {
    const fileRecords = successfulUploads.map(result => ({
      id: result.fileId,
      mocId: mocId,
      fileName: result.filename,
      fileUrl: result.s3Url!,
      fileType: getMimeType(result.filename),
      fileSize: getFileSize(result.filename, files),
      uploadedBy: userId,
      uploadedAt: new Date(),
    }))

    // Batch insert all successful files
    await tx.insert(mocFiles).values(fileRecords)
  })
}
```

### Partial Success Response

```typescript
interface MultiFileUploadResponse {
  uploaded: UploadedFile[]
  failed: FailedFile[]
  summary: {
    total: number
    succeeded: number
    failed: number
  }
}

interface UploadedFile {
  id: string
  filename: string
  fileUrl: string
  fileSize: number
}

interface FailedFile {
  filename: string
  error: string
}

// Build response
const uploaded = uploadResults
  .filter(r => r.success)
  .map(r => ({
    id: r.fileId!,
    filename: r.filename,
    fileUrl: r.s3Url!,
    fileSize: getFileSize(r.filename, files),
  }))

const failed = uploadResults
  .filter(r => !r.success)
  .map(r => ({
    filename: r.filename,
    error: r.error!,
  }))

return createSuccessResponse({
  uploaded,
  failed,
  summary: {
    total: files.length,
    succeeded: uploaded.length,
    failed: failed.length,
  },
}, 200)
```

### Complete Handler Implementation

```typescript
async function handleMultiFileUpload(
  event: APIGatewayProxyEventV2,
  userId: string,
  mocId: string,
): Promise<APIGatewayProxyResultV2> {
  // 1. Verify MOC exists and user owns it
  const [moc] = await db.select().from(mocInstructions).where(eq(mocInstructions.id, mocId))

  if (!moc) {
    return createErrorResponse(404, 'NOT_FOUND', 'MOC not found')
  }

  if (moc.userId !== userId) {
    return createErrorResponse(403, 'FORBIDDEN', 'Cannot upload to another user\'s MOC')
  }

  // 2. Parse multipart form data
  const formData = await parseMultipartForm(event)
  const files = formData.filter(part => part.type === 'file')

  // 3. Validate file count and total size
  if (files.length === 0) {
    return createErrorResponse(400, 'BAD_REQUEST', 'No files provided')
  }

  if (files.length > 10) {
    return createErrorResponse(400, 'VALIDATION_ERROR', 'Maximum 10 files per upload')
  }

  const totalSize = files.reduce((sum, f) => sum + f.buffer.length, 0)
  if (totalSize > 50 * 1024 * 1024) {
    return createErrorResponse(400, 'VALIDATION_ERROR', 'Total payload exceeds 50MB')
  }

  // 4. Upload files in parallel (includes individual validation)
  const uploadResults = await uploadFilesParallel(files, mocId, userId)

  // 5. Insert successful uploads to database
  const successfulUploads = uploadResults.filter(r => r.success)

  if (successfulUploads.length > 0) {
    await insertFilesToDatabase(successfulUploads, mocId, userId, files)
  }

  // 6. Build and return response
  return buildMultiFileResponse(uploadResults, files)
}
```

### Lambda Configuration

```typescript
// sst.config.ts - Enhance existing MOC file upload function
const mocFileUploadFunction = new sst.aws.Function('MocFileUploadFunction', {
  handler: 'src/functions/moc-file-upload.handler',
  runtime: 'nodejs20.x',
  timeout: '120 seconds', // 2 minutes for multi-file uploads
  memory: '2048 MB',      // Increased for parallel processing
  vpc,
  link: [postgres, bucket],
  environment: {
    NODE_ENV: stage === 'production' ? 'production' : 'development',
    STAGE: stage,
    MAX_FILES_PER_UPLOAD: '10',
    MAX_TOTAL_PAYLOAD: '52428800', // 50MB in bytes
  },
})
```

### Response Format

```json
{
  "success": true,
  "data": {
    "uploaded": [
      {
        "id": "uuid-1",
        "filename": "instructions-part1.pdf",
        "fileUrl": "https://bucket.s3.amazonaws.com/...",
        "fileSize": 2548736
      },
      {
        "id": "uuid-2",
        "filename": "instructions-part2.pdf",
        "fileUrl": "https://bucket.s3.amazonaws.com/...",
        "fileSize": 1894562
      }
    ],
    "failed": [
      {
        "filename": "invalid.exe",
        "error": "File type not allowed"
      }
    ],
    "summary": {
      "total": 3,
      "succeeded": 2,
      "failed": 1
    }
  },
  "timestamp": "2025-01-02T12:00:00Z"
}
```

---

## Design Decisions

### Parallel Upload with Promise.all()

**Decision**: Upload all files to S3 in parallel

**Rationale**:
- Dramatically faster than sequential uploads (10x speedup for 10 files)
- S3 can handle high concurrency
- Lambda has 2048 MB memory to support parallel operations
- Individual failures don't block other uploads

**Risk Mitigation**: Individual try-catch for each upload promise

### Partial Success Model

**Decision**: Return success even if some files fail

**Rationale**:
- User doesn't have to re-upload successful files
- Clear error messages help fix specific issues
- Common in batch operations (e.g., email services)
- Better UX than all-or-nothing

**Alternative Rejected**: All-or-nothing (too strict, poor UX)

### Database Transaction for Successful Only

**Decision**: Insert only successful uploads in database

**Rationale**:
- Database state matches S3 state (consistency)
- Failed uploads don't create orphaned records
- Transaction ensures atomicity of successful batch
- Rollback protection if database insert fails

### 50 MB Total Limit

**Decision**: Limit total payload to 50 MB

**Rationale**:
- API Gateway max payload is 10 MB (use S3 presigned for larger)
- Lambda can handle 50 MB in memory with 2048 MB allocation
- Prevents memory exhaustion
- Encourages reasonable batch sizes

**Future Enhancement**: Use S3 presigned URLs for >50 MB batches

---

## Error Scenarios

| Scenario | Status Code | Response |
|----------|-------------|----------|
| All files successful | 200 | `{ uploaded: [10], failed: [] }` |
| Partial success | 200 | `{ uploaded: [7], failed: [3] }` |
| All files failed | 200 | `{ uploaded: [], failed: [10] }` |
| No files | 400 | BAD_REQUEST |
| >10 files | 400 | VALIDATION_ERROR |
| >50 MB total | 400 | VALIDATION_ERROR |
| MOC not found | 404 | NOT_FOUND |
| Not MOC owner | 403 | FORBIDDEN |

**Note**: 200 returned even with partial/complete failures (failures detailed in response body)

---

## Performance Considerations

1. **Parallel Upload**: 10 files upload in ~10-30 seconds (vs 60-180 sequential)
2. **Memory**: 2048 MB handles 10x 5MB files comfortably
3. **Timeout**: 120 seconds allows for network variability
4. **Database**: Batch insert more efficient than 10 individual inserts

**Benchmark Targets**:
- 5 files @ 5MB each: <30 seconds total
- 10 files @ 3MB each: <45 seconds total
- Network variance: Plan for 2x on slow connections

---

## Client-Side Implementation

```typescript
// Frontend example using FormData
const formData = new FormData()
files.forEach(file => {
  formData.append('files', file)
})

const response = await fetch(`/api/mocs/${mocId}/files`, {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${token}`,
  },
  body: formData,
})

const result = await response.json()

// Handle partial success
if (result.data.failed.length > 0) {
  console.warn('Some files failed:', result.data.failed)
  // Show user which files need re-upload
}

// Update UI with uploaded files
result.data.uploaded.forEach(file => {
  addFileToList(file)
})
```

---

## Dependencies

- **Story 2.1**: MOC Instructions Lambda (provides MOC CRUD)
- **Story 2.7**: MOC File Upload (extends this handler)
- **@monorepo/file-validator**: For file validation
- **S3 Bucket**: For file storage
- **PostgreSQL**: For file metadata

---

## Future Enhancements

1. **Progress Tracking**: WebSocket updates for each file
2. **Resumable Uploads**: Use S3 multipart upload for very large files
3. **Chunked Upload**: Split large files into chunks
4. **Client-Side Validation**: Pre-validate before upload
5. **Retry Failed**: Auto-retry failed uploads (exponential backoff)
6. **Presigned URLs**: For files >50 MB (bypass Lambda limits)
