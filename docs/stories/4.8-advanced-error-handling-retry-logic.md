# Story 4.8: Implement Advanced Error Handling and Retry Logic

**Epic**: 4 - User Profile & Advanced Features Migration

**As a** backend developer,
**I want** robust error handling with automatic retries for transient failures,
**so that** users experience reliable service even during AWS service hiccups.

## Acceptance Criteria

1. All Lambda functions implement structured error handling with custom error classes
2. Transient errors (network timeouts, throttling) trigger exponential backoff retry (max 3 attempts)
3. Non-retryable errors (validation, authorization) fail immediately with clear messages
4. Database connection errors trigger retry with jitter
5. S3 upload failures logged with presigned URL fallback notification to user
6. OpenSearch indexing failures logged but do not block main operation (eventual consistency acceptable)
7. All errors logged to CloudWatch with structured JSON format including: `errorType`, `errorMessage`, `requestId`, `userId`
8. Error responses never expose internal implementation details (sanitized messages)
9. AWS X-Ray tracing enabled to track error propagation across services
10. CloudWatch metric alarms configured for error rate thresholds (>5% error rate triggers alert)

## Implementation Status

**Status**: Not Started

## Files Modified

_To be populated during implementation_

## QA Results

_Pending implementation and review_

---

## Requirements Traceability Matrix

| AC # | Requirement                            | Test Coverage | Status   |
| ---- | -------------------------------------- | ------------- | -------- |
| 1    | Custom error classes                   | TBD           | ⏳ PENDING |
| 2    | Transient error retry (exponential)    | TBD           | ⏳ PENDING |
| 3    | Non-retryable errors fail fast         | TBD           | ⏳ PENDING |
| 4    | Database retry with jitter             | TBD           | ⏳ PENDING |
| 5    | S3 failure logging + fallback          | TBD           | ⏳ PENDING |
| 6    | OpenSearch non-blocking failures       | TBD           | ⏳ PENDING |
| 7    | Structured CloudWatch logging          | TBD           | ⏳ PENDING |
| 8    | Sanitized error messages               | TBD           | ⏳ PENDING |
| 9    | AWS X-Ray tracing enabled              | TBD           | ⏳ PENDING |
| 10   | CloudWatch alarms (>5% error rate)     | TBD           | ⏳ PENDING |

**Overall Coverage: TBD**

---

## Test Summary

_To be populated during implementation_

### Recommended Test Coverage

1. **Custom Error Classes** - 5 tests
   - ValidationError properly constructed
   - AuthorizationError properly constructed
   - DatabaseError properly constructed
   - ExternalServiceError properly constructed
   - Error inheritance chain verified

2. **Retry Logic** - 8 tests
   - Transient error retries 3 times
   - Exponential backoff timing verified
   - Success on 2nd retry
   - Non-retryable error fails immediately
   - Database retry with jitter
   - Max retries exhausted → error
   - Retry counter logged correctly
   - Jitter prevents thundering herd

3. **Error Sanitization** - 4 tests
   - Database errors don't expose SQL
   - AWS errors don't expose internal details
   - Stack traces not in client response
   - Safe error messages only

4. **Logging** - 3 tests
   - Structured JSON format verified
   - All required fields present
   - CloudWatch integration working

5. **X-Ray** - 2 tests
   - Traces captured for requests
   - Error traces include subsegments

---

## Technical Notes

### Custom Error Classes

```typescript
// src/lib/errors/custom-errors.ts

export class AppError extends Error {
  constructor(
    public statusCode: number,
    public errorType: string,
    message: string,
    public isRetryable: boolean = false,
    public details?: Record<string, unknown>
  ) {
    super(message)
    this.name = this.constructor.name
    Error.captureStackTrace(this, this.constructor)
  }

  toJSON() {
    return {
      statusCode: this.statusCode,
      errorType: this.errorType,
      message: this.message,
      details: this.details,
    }
  }
}

export class ValidationError extends AppError {
  constructor(message: string, details?: Record<string, unknown>) {
    super(400, 'VALIDATION_ERROR', message, false, details)
  }
}

export class AuthorizationError extends AppError {
  constructor(message: string = 'Unauthorized access') {
    super(403, 'FORBIDDEN', message, false)
  }
}

export class DatabaseError extends AppError {
  constructor(message: string = 'Database operation failed', isRetryable: boolean = true) {
    super(500, 'DATABASE_ERROR', message, isRetryable)
  }
}

export class ExternalServiceError extends AppError {
  constructor(
    public service: string,
    message: string,
    isRetryable: boolean = true
  ) {
    super(500, 'EXTERNAL_SERVICE_ERROR', message, isRetryable)
  }
}

export class ThrottlingError extends AppError {
  constructor(service: string) {
    super(429, 'TOO_MANY_REQUESTS', `${service} rate limit exceeded`, true)
  }
}
```

### Exponential Backoff Retry

```typescript
// src/lib/utils/retry.ts

interface RetryOptions {
  maxAttempts?: number
  baseDelay?: number
  maxDelay?: number
  exponentialBase?: number
  jitter?: boolean
}

const defaultOptions: Required<RetryOptions> = {
  maxAttempts: 3,
  baseDelay: 100,        // 100ms
  maxDelay: 5000,        // 5 seconds
  exponentialBase: 2,
  jitter: true,
}

export async function retryWithBackoff<T>(
  operation: () => Promise<T>,
  options: RetryOptions = {}
): Promise<T> {
  const opts = { ...defaultOptions, ...options }
  let lastError: Error

  for (let attempt = 1; attempt <= opts.maxAttempts; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error as Error

      // Don't retry if not retryable
      if (error instanceof AppError && !error.isRetryable) {
        throw error
      }

      // Don't retry on last attempt
      if (attempt === opts.maxAttempts) {
        throw error
      }

      // Calculate delay with exponential backoff
      const exponentialDelay = opts.baseDelay * Math.pow(opts.exponentialBase, attempt - 1)
      let delay = Math.min(exponentialDelay, opts.maxDelay)

      // Add jitter to prevent thundering herd
      if (opts.jitter) {
        delay = delay * (0.5 + Math.random() * 0.5) // 50-100% of calculated delay
      }

      console.warn(`Retry attempt ${attempt}/${opts.maxAttempts} after ${delay}ms`, {
        error: error.message,
        attempt,
        delay,
      })

      await sleep(delay)
    }
  }

  throw lastError!
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}
```

### Database Retry with Jitter

```typescript
// src/lib/db/client-with-retry.ts

import { retryWithBackoff } from '@/lib/utils/retry'
import { DatabaseError } from '@/lib/errors/custom-errors'

export async function queryWithRetry<T>(
  queryFn: () => Promise<T>
): Promise<T> {
  return retryWithBackoff(
    async () => {
      try {
        return await queryFn()
      } catch (error) {
        // Determine if error is retryable
        const isRetryable = isDatabaseErrorRetryable(error)
        throw new DatabaseError(
          'Database query failed',
          isRetryable
        )
      }
    },
    {
      maxAttempts: 3,
      baseDelay: 200,
      maxDelay: 2000,
      jitter: true, // Prevents thundering herd on DB
    }
  )
}

function isDatabaseErrorRetryable(error: any): boolean {
  // PostgreSQL error codes for retryable errors
  const retryableCodes = [
    '40001', // serialization_failure
    '40P01', // deadlock_detected
    '53000', // insufficient_resources
    '53100', // disk_full
    '53200', // out_of_memory
    '53300', // too_many_connections
  ]

  return retryableCodes.includes(error.code)
}
```

### S3 Upload with Fallback

```typescript
// src/lib/storage/s3-client-with-retry.ts

export async function uploadToS3WithRetry(params: UploadParams): Promise<string> {
  try {
    return await retryWithBackoff(
      () => uploadToS3(params),
      {
        maxAttempts: 3,
        baseDelay: 500,
        exponentialBase: 2,
      }
    )
  } catch (error) {
    console.error('S3 upload failed after retries', {
      key: params.key,
      error: error.message,
      attemptedRetries: 3,
    })

    // Generate presigned URL as fallback
    const presignedUrl = await generatePresignedUploadUrl(params.key)

    throw new ExternalServiceError(
      'S3',
      'Upload failed. Please use the presigned URL provided.',
      false // Not retryable at this point
    ).withDetails({ presignedUrl })
  }
}
```

### OpenSearch Non-Blocking

```typescript
// src/lib/search/opensearch-client.ts

export async function indexDocumentNonBlocking(params: IndexParams): Promise<void> {
  try {
    await retryWithBackoff(
      () => indexDocument(params),
      {
        maxAttempts: 2, // Fewer retries since non-critical
        baseDelay: 300,
      }
    )
  } catch (error) {
    // Log error but don't throw - eventual consistency is acceptable
    console.error('OpenSearch indexing failed (non-blocking)', {
      index: params.index,
      id: params.id,
      error: error.message,
    })

    // TODO: Consider dead letter queue for manual reprocessing
  }
}
```

### Structured CloudWatch Logging

```typescript
// src/lib/logging/logger.ts

interface LogContext {
  requestId: string
  userId?: string
  mocId?: string
  [key: string]: unknown
}

export function logError(
  error: Error | AppError,
  context: LogContext,
  additionalInfo?: Record<string, unknown>
): void {
  const logEntry = {
    timestamp: new Date().toISOString(),
    level: 'ERROR',
    errorType: error instanceof AppError ? error.errorType : 'INTERNAL_ERROR',
    errorMessage: error.message,
    errorName: error.name,
    statusCode: error instanceof AppError ? error.statusCode : 500,
    isRetryable: error instanceof AppError ? error.isRetryable : false,
    requestId: context.requestId,
    userId: context.userId,
    mocId: context.mocId,
    stack: process.env.NODE_ENV === 'development' ? error.stack : undefined,
    ...additionalInfo,
  }

  console.error(JSON.stringify(logEntry))
}

export function logInfo(message: string, context: LogContext): void {
  const logEntry = {
    timestamp: new Date().toISOString(),
    level: 'INFO',
    message,
    ...context,
  }

  console.log(JSON.stringify(logEntry))
}
```

### Error Sanitization

```typescript
// src/lib/utils/error-sanitizer.ts

export function sanitizeError(error: Error | AppError): {
  statusCode: number
  errorType: string
  message: string
  details?: Record<string, unknown>
} {
  // Custom app errors are already sanitized
  if (error instanceof AppError) {
    return {
      statusCode: error.statusCode,
      errorType: error.errorType,
      message: error.message,
      details: error.details,
    }
  }

  // AWS SDK errors
  if (error.name?.includes('AWS')) {
    return {
      statusCode: 500,
      errorType: 'EXTERNAL_SERVICE_ERROR',
      message: 'An external service error occurred',
      // Don't expose AWS error details
    }
  }

  // Database errors
  if (error.name === 'PostgresError' || error.message?.includes('SQL')) {
    return {
      statusCode: 500,
      errorType: 'DATABASE_ERROR',
      message: 'A database error occurred',
      // Don't expose SQL details
    }
  }

  // Generic fallback
  return {
    statusCode: 500,
    errorType: 'INTERNAL_ERROR',
    message: 'An unexpected error occurred',
  }
}
```

### AWS X-Ray Tracing

```typescript
// src/lib/tracing/xray.ts

import AWSXRay from 'aws-xray-sdk-core'
import AWS from 'aws-sdk'

// Wrap AWS SDK with X-Ray
const instrumentedAWS = AWSXRay.captureAWS(AWS)

export function traceAsyncOperation<T>(
  name: string,
  operation: () => Promise<T>
): Promise<T> {
  const segment = AWSXRay.getSegment()

  if (!segment) {
    // X-Ray not enabled, run without tracing
    return operation()
  }

  const subsegment = segment.addNewSubsegment(name)

  return operation()
    .then(result => {
      subsegment.close()
      return result
    })
    .catch(error => {
      subsegment.addError(error)
      subsegment.close()
      throw error
    })
}

// Usage example
export async function uploadImageWithTracing(file: Buffer): Promise<string> {
  return traceAsyncOperation('uploadImage', async () => {
    const processed = await traceAsyncOperation('processImage', () =>
      processImage(file)
    )

    const url = await traceAsyncOperation('uploadToS3', () =>
      uploadToS3(processed)
    )

    return url
  })
}
```

### Lambda Handler Error Wrapper

```typescript
// src/lib/utils/lambda-wrapper.ts

export function withErrorHandling(
  handler: (event: APIGatewayProxyEventV2) => Promise<APIGatewayProxyResultV2>
): (event: APIGatewayProxyEventV2) => Promise<APIGatewayProxyResultV2> {
  return async (event: APIGatewayProxyEventV2) => {
    const requestId = event.requestContext.requestId
    const userId = getUserIdFromEvent(event)

    try {
      return await handler(event)
    } catch (error) {
      // Log structured error
      logError(error as Error, { requestId, userId })

      // Sanitize and return error response
      const sanitized = sanitizeError(error as Error)

      return {
        statusCode: sanitized.statusCode,
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          success: false,
          error: {
            type: sanitized.errorType,
            message: sanitized.message,
            details: sanitized.details,
          },
          timestamp: new Date().toISOString(),
        }),
      }
    }
  }
}

// Usage in handler
export const handler = withErrorHandling(async (event) => {
  // Handler implementation
  // Errors automatically caught, logged, and sanitized
})
```

### CloudWatch Alarms Configuration

```typescript
// sst.config.ts

import * as cloudwatch from '@aws-cdk/aws-cloudwatch'

// Error rate alarm for each Lambda
const errorRateAlarm = new cloudwatch.Alarm(this, 'ErrorRateAlarm', {
  alarmName: `${functionName}-error-rate`,
  metric: new cloudwatch.Metric({
    namespace: 'AWS/Lambda',
    metricName: 'Errors',
    dimensions: {
      FunctionName: functionName,
    },
    statistic: 'Sum',
    period: cdk.Duration.minutes(5),
  }),
  threshold: 5, // 5% error rate
  evaluationPeriods: 2,
  comparisonOperator: cloudwatch.ComparisonOperator.GREATER_THAN_THRESHOLD,
  treatMissingData: cloudwatch.TreatMissingData.NOT_BREACHING,
})

// Configure SNS topic for alerts
const alertTopic = new sns.Topic(this, 'ErrorAlertTopic')
errorRateAlarm.addAlarmAction(new cloudwatch_actions.SnsAction(alertTopic))
```

---

## Design Decisions

### Exponential Backoff with Jitter

**Decision**: Use exponential backoff with jitter for retries

**Rationale**:
- Exponential backoff prevents overwhelming failing services
- Jitter prevents thundering herd (many clients retrying simultaneously)
- Industry best practice (AWS SDK default behavior)

**Formula**: `delay = baseDelay * 2^(attempt-1) * random(0.5, 1.0)`

### Max 3 Retry Attempts

**Decision**: Limit retries to 3 attempts

**Rationale**:
- Balance between reliability and latency
- Most transient issues resolve within 2-3 retries
- Prevents infinite loops
- Keeps within Lambda timeout constraints

### Non-Blocking OpenSearch

**Decision**: OpenSearch failures don't block main operations

**Rationale**:
- Search is non-critical (can be added later)
- Eventual consistency is acceptable for search
- Prevents search issues from breaking core functionality
- Failed indexing can be retried via background job

### Sanitized Error Messages

**Decision**: Never expose internal details in error responses

**Rationale**:
- Security: Prevents information leakage
- UX: Technical errors confuse users
- Logging: Full details captured in CloudWatch for debugging
- Compliance: Prevents exposing sensitive data

---

## Error Categorization

| Error Type | Retryable? | Max Attempts | Base Delay | Example |
|------------|-----------|--------------|------------|---------|
| Validation | ❌ No | N/A | N/A | Invalid input |
| Authorization | ❌ No | N/A | N/A | Access denied |
| Not Found | ❌ No | N/A | N/A | Resource missing |
| Database Connection | ✅ Yes | 3 | 200ms | Connection timeout |
| Database Deadlock | ✅ Yes | 3 | 200ms | Transaction conflict |
| S3 Throttling | ✅ Yes | 3 | 500ms | Rate limit |
| OpenSearch Timeout | ✅ Yes | 2 | 300ms | Network timeout |
| Cognito Throttling | ✅ Yes | 3 | 500ms | TooManyRequestsException |

---

## CloudWatch Dashboard

Recommended metrics to monitor:

1. **Error Rate**: Errors / Invocations (%)
2. **Retry Count**: Average retries per request
3. **Error Types**: Breakdown by errorType
4. **P99 Latency**: 99th percentile response time
5. **Throttles**: ThrottlingError count
6. **Database Errors**: DatabaseError count

---

## Dependencies

- **All Previous Stories**: This is a cross-cutting concern applied to all Lambda functions
- **AWS X-Ray**: For distributed tracing
- **CloudWatch Logs**: For structured logging
- **CloudWatch Alarms**: For error monitoring

---

## Testing Strategy

### Unit Tests

```typescript
describe('retryWithBackoff', () => {
  it('should retry transient errors', async () => {
    let attempts = 0
    const operation = jest.fn(async () => {
      attempts++
      if (attempts < 3) {
        throw new ThrottlingError('S3')
      }
      return 'success'
    })

    const result = await retryWithBackoff(operation)

    expect(result).toBe('success')
    expect(attempts).toBe(3)
  })

  it('should not retry validation errors', async () => {
    const operation = jest.fn(async () => {
      throw new ValidationError('Invalid input')
    })

    await expect(retryWithBackoff(operation)).rejects.toThrow(ValidationError)
    expect(operation).toHaveBeenCalledTimes(1) // Only once
  })
})
```

### Integration Tests

```typescript
describe('Database retry integration', () => {
  it('should retry on connection timeout', async () => {
    // Simulate connection timeout
    mockDatabase.query.mockRejectedValueOnce({ code: '53300' }) // too_many_connections
    mockDatabase.query.mockResolvedValueOnce([{ id: '123' }])

    const result = await queryWithRetry(() => db.select().from(users))

    expect(result).toEqual([{ id: '123' }])
    expect(mockDatabase.query).toHaveBeenCalledTimes(2)
  })
})
```

---

## Future Enhancements

1. **Circuit Breaker**: Stop retrying if service consistently fails
2. **Rate Limiting**: Implement client-side rate limiting
3. **Dead Letter Queue**: For failed operations requiring manual intervention
4. **Retry Budget**: Limit retries across all requests to prevent cascading failures
5. **Adaptive Timeouts**: Adjust timeouts based on P99 latency
