# Story 5.6: Performance and Cost Validation Post-Migration

**Epic**: 5 - Production Deployment, Monitoring & Cutover

**As a** DevOps engineer,
**I want** to validate performance and cost of SST deployment,
**so that** I can confirm the migration meets targets before decommissioning ECS.

## Acceptance Criteria

1. Performance benchmarks run comparing ECS vs SST (response time, throughput)
2. Load testing with 1000 concurrent users for 10 minutes
3. API response time p95 <500ms (same as ECS baseline)
4. API throughput >100 requests/second (same as ECS baseline)
5. Cost analysis: SST monthly cost ‚â§ ECS monthly cost
6. Cold start analysis: p99 cold start <2 seconds
7. Database connection pool validation (no connection exhaustion)
8. Redis cache hit rate >80% (same as ECS baseline)
9. S3 operation latency <200ms (same as ECS baseline)
10. OpenSearch query latency p95 <300ms (same as ECS baseline)
11. Validation report generated with pass/fail for each metric

## Implementation Status

**Status**: Not Started

## Files Modified

_To be populated during implementation_

## QA Results

_Pending implementation and review_

---

## Requirements Traceability Matrix

| AC # | Requirement                            | Test Coverage | Status   |
| ---- | -------------------------------------- | ------------- | -------- |
| 1    | Performance benchmarks (ECS vs SST)    | TBD           | ‚è≥ PENDING |
| 2    | Load testing (1000 users, 10 min)      | TBD           | ‚è≥ PENDING |
| 3    | API response time p95 <500ms           | TBD           | ‚è≥ PENDING |
| 4    | API throughput >100 req/sec            | TBD           | ‚è≥ PENDING |
| 5    | Cost analysis (SST ‚â§ ECS)              | TBD           | ‚è≥ PENDING |
| 6    | Cold start p99 <2s                     | TBD           | ‚è≥ PENDING |
| 7    | Database connection pool validation    | TBD           | ‚è≥ PENDING |
| 8    | Redis cache hit rate >80%              | TBD           | ‚è≥ PENDING |
| 9    | S3 latency <200ms                      | TBD           | ‚è≥ PENDING |
| 10   | OpenSearch latency p95 <300ms          | TBD           | ‚è≥ PENDING |
| 11   | Validation report generated            | TBD           | ‚è≥ PENDING |

**Overall Coverage: TBD**

---

## Test Summary

_To be populated during implementation_

### Recommended Test Coverage

1. **Load Testing** - 5 tests
   - 100 concurrent users (baseline)
   - 500 concurrent users (moderate load)
   - 1000 concurrent users (peak load)
   - Sustained load for 10 minutes
   - Spike test (sudden traffic increase)

2. **Performance Validation** - 4 tests
   - Response time percentiles (p50, p95, p99)
   - Throughput measurement (requests/second)
   - Cold start analysis (first invocation per Lambda)
   - Database query performance

3. **Cost Validation** - 2 tests
   - 7-day cost projection based on current usage
   - 30-day cost comparison (ECS vs SST)

4. **Integration Validation** - 3 tests
   - Redis cache hit rate
   - S3 operation latency
   - OpenSearch query latency

---

## Technical Notes

### Load Testing with Artillery

```yaml
# tests/performance/load-test.yml
config:
  target: "https://api.lego-mocs.com"
  phases:
    # Warm-up phase
    - duration: 60
      arrivalRate: 10
      name: "Warm-up"

    # Ramp-up phase
    - duration: 120
      arrivalRate: 10
      rampTo: 100
      name: "Ramp-up to 100 users"

    # Sustained load phase
    - duration: 600
      arrivalRate: 100
      name: "Sustained load (1000 concurrent users)"

    # Ramp-down phase
    - duration: 60
      arrivalRate: 100
      rampTo: 10
      name: "Ramp-down"

  processor: "./test-processor.js"

  variables:
    authToken: "Bearer {{ $processEnvironment.TEST_AUTH_TOKEN }}"

scenarios:
  # Scenario 1: List MOCs
  - name: "List MOCs"
    weight: 40
    flow:
      - get:
          url: "/api/mocs"
          headers:
            Authorization: "{{ authToken }}"
          capture:
            - json: "$.data[0].id"
              as: "mocId"
          expect:
            - statusCode: 200
            - contentType: json

  # Scenario 2: Get MOC by ID
  - name: "Get MOC by ID"
    weight: 30
    flow:
      - get:
          url: "/api/mocs/{{ mocId }}"
          headers:
            Authorization: "{{ authToken }}"
          expect:
            - statusCode: 200
            - contentType: json

  # Scenario 3: Search Gallery
  - name: "Search Gallery"
    weight: 20
    flow:
      - get:
          url: "/api/gallery?search=lego"
          headers:
            Authorization: "{{ authToken }}"
          expect:
            - statusCode: 200
            - contentType: json

  # Scenario 4: Get Wishlist
  - name: "Get Wishlist"
    weight: 10
    flow:
      - get:
          url: "/api/wishlist"
          headers:
            Authorization: "{{ authToken }}"
          expect:
            - statusCode: 200
            - contentType: json

  # Scenario 5: Upload Image (stress test)
  - name: "Upload Image"
    weight: 5
    flow:
      - post:
          url: "/api/gallery/{{ galleryImageId }}/image"
          headers:
            Authorization: "{{ authToken }}"
            Content-Type: "multipart/form-data"
          formData:
            image:
              path: "./fixtures/test-image.jpg"
          expect:
            - statusCode: 200
```

### Artillery Test Runner Script

```bash
#!/bin/bash
# scripts/run-performance-tests.sh

set -e

echo "üöÄ Starting performance validation tests..."

# Step 1: Run load test
echo "üìä Running Artillery load test..."
artillery run tests/performance/load-test.yml \
  --output results/artillery-report.json

# Step 2: Generate HTML report
echo "üìà Generating HTML report..."
artillery report results/artillery-report.json \
  --output results/artillery-report.html

# Step 3: Extract key metrics
echo "üìã Extracting key metrics..."
node scripts/extract-metrics.js results/artillery-report.json

# Step 4: Compare with baseline
echo "üîç Comparing with ECS baseline..."
node scripts/compare-baselines.js results/metrics.json baselines/ecs-baseline.json

# Step 5: Validate thresholds
echo "‚úÖ Validating performance thresholds..."
node scripts/validate-thresholds.js results/metrics.json

echo "‚úÖ Performance validation complete!"
```

### Metrics Extraction Script

```typescript
// scripts/extract-metrics.ts
import fs from 'fs'

interface ArtilleryReport {
  aggregate: {
    counters: {
      'http.requests': number
      'http.responses': number
      'http.codes.200': number
      'http.codes.500': number
    }
    rates: {
      'http.request_rate': number
    }
    summaries: {
      'http.response_time': {
        min: number
        max: number
        median: number
        p95: number
        p99: number
      }
    }
  }
}

const reportPath = process.argv[2]
const report: ArtilleryReport = JSON.parse(fs.readFileSync(reportPath, 'utf-8'))

const metrics = {
  totalRequests: report.aggregate.counters['http.requests'],
  totalResponses: report.aggregate.counters['http.responses'],
  successfulRequests: report.aggregate.counters['http.codes.200'],
  failedRequests: report.aggregate.counters['http.codes.500'] || 0,
  requestRate: report.aggregate.rates['http.request_rate'],
  responseTime: {
    min: report.aggregate.summaries['http.response_time'].min,
    max: report.aggregate.summaries['http.response_time'].max,
    median: report.aggregate.summaries['http.response_time'].median,
    p95: report.aggregate.summaries['http.response_time'].p95,
    p99: report.aggregate.summaries['http.response_time'].p99,
  },
  errorRate: (report.aggregate.counters['http.codes.500'] || 0) / report.aggregate.counters['http.requests'] * 100,
}

fs.writeFileSync('results/metrics.json', JSON.stringify(metrics, null, 2))

console.log('Extracted Metrics:')
console.log(`  Total Requests: ${metrics.totalRequests}`)
console.log(`  Successful Requests: ${metrics.successfulRequests}`)
console.log(`  Failed Requests: ${metrics.failedRequests}`)
console.log(`  Request Rate: ${metrics.requestRate.toFixed(2)} req/sec`)
console.log(`  Response Time (p95): ${metrics.responseTime.p95.toFixed(2)} ms`)
console.log(`  Response Time (p99): ${metrics.responseTime.p99.toFixed(2)} ms`)
console.log(`  Error Rate: ${metrics.errorRate.toFixed(2)}%`)
```

### Threshold Validation Script

```typescript
// scripts/validate-thresholds.ts
import fs from 'fs'

interface Metrics {
  totalRequests: number
  successfulRequests: number
  failedRequests: number
  requestRate: number
  responseTime: {
    p95: number
    p99: number
  }
  errorRate: number
}

const metricsPath = process.argv[2]
const metrics: Metrics = JSON.parse(fs.readFileSync(metricsPath, 'utf-8'))

interface Threshold {
  name: string
  value: number
  threshold: number
  comparator: '<' | '>' | '<=' | '>='
  pass: boolean
}

const thresholds: Threshold[] = [
  {
    name: 'Response Time p95',
    value: metrics.responseTime.p95,
    threshold: 500,
    comparator: '<',
    pass: metrics.responseTime.p95 < 500,
  },
  {
    name: 'Response Time p99',
    value: metrics.responseTime.p99,
    threshold: 2000,
    comparator: '<',
    pass: metrics.responseTime.p99 < 2000,
  },
  {
    name: 'Request Rate',
    value: metrics.requestRate,
    threshold: 100,
    comparator: '>',
    pass: metrics.requestRate > 100,
  },
  {
    name: 'Error Rate',
    value: metrics.errorRate,
    threshold: 1,
    comparator: '<',
    pass: metrics.errorRate < 1,
  },
]

console.log('\n=== Threshold Validation ===\n')

let allPassed = true

thresholds.forEach((threshold) => {
  const status = threshold.pass ? '‚úÖ PASS' : '‚ùå FAIL'
  console.log(`${status} ${threshold.name}: ${threshold.value.toFixed(2)} ${threshold.comparator} ${threshold.threshold}`)

  if (!threshold.pass) {
    allPassed = false
  }
})

if (!allPassed) {
  console.log('\n‚ùå Validation FAILED: One or more thresholds not met.')
  process.exit(1)
} else {
  console.log('\n‚úÖ Validation PASSED: All thresholds met.')
}
```

### ECS Baseline Metrics

```json
{
  "testDate": "2025-01-01",
  "environment": "ECS Production",
  "totalRequests": 100000,
  "successfulRequests": 99500,
  "failedRequests": 500,
  "requestRate": 166.67,
  "responseTime": {
    "min": 45,
    "max": 2500,
    "median": 250,
    "p95": 450,
    "p99": 800
  },
  "errorRate": 0.5,
  "cacheHitRate": 85,
  "s3Latency": {
    "p95": 180
  },
  "openSearchLatency": {
    "p95": 280
  },
  "databaseConnections": {
    "max": 50,
    "average": 25
  },
  "monthlyCost": {
    "ecs": 450,
    "rds": 200,
    "elasticache": 100,
    "elasticsearch": 150,
    "s3": 50,
    "total": 950
  }
}
```

### Cost Analysis Script

```typescript
// scripts/analyze-costs.ts
import { CostExplorerClient, GetCostAndUsageCommand } from '@aws-sdk/client-cost-explorer'

const costExplorer = new CostExplorerClient({ region: 'us-east-1' })

async function getSST Cost(startDate: string, endDate: string) {
  const response = await costExplorer.send(new GetCostAndUsageCommand({
    TimePeriod: {
      Start: startDate,
      End: endDate,
    },
    Granularity: 'MONTHLY',
    Metrics: ['UnblendedCost'],
    GroupBy: [
      {
        Type: 'DIMENSION',
        Key: 'SERVICE',
      },
    ],
    Filter: {
      Tags: {
        Key: 'Project',
        Values: ['lego-api-sst'],
      },
    },
  }))

  const costs: Record<string, number> = {}
  let totalCost = 0

  response.ResultsByTime?.forEach((result) => {
    result.Groups?.forEach((group) => {
      const service = group.Keys?.[0] || 'Unknown'
      const cost = parseFloat(group.Metrics?.UnblendedCost?.Amount || '0')
      costs[service] = cost
      totalCost += cost
    })
  })

  return { costs, totalCost }
}

async function main() {
  const endDate = new Date().toISOString().split('T')[0]
  const startDate = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString().split('T')[0]

  const { costs, totalCost } = await getSSTCost(startDate, endDate)

  console.log('\n=== SST Cost Analysis (7 days) ===\n')
  console.log(`Lambda: $${costs['AWS Lambda']?.toFixed(2) || '0.00'}`)
  console.log(`API Gateway: $${costs['Amazon API Gateway']?.toFixed(2) || '0.00'}`)
  console.log(`RDS: $${costs['Amazon Relational Database Service']?.toFixed(2) || '0.00'}`)
  console.log(`ElastiCache: $${costs['Amazon ElastiCache']?.toFixed(2) || '0.00'}`)
  console.log(`OpenSearch: $${costs['Amazon OpenSearch Service']?.toFixed(2) || '0.00'}`)
  console.log(`S3: $${costs['Amazon Simple Storage Service']?.toFixed(2) || '0.00'}`)
  console.log(`\nTotal (7 days): $${totalCost.toFixed(2)}`)

  // Project to 30 days
  const monthlyProjection = (totalCost / 7) * 30
  console.log(`Monthly Projection: $${monthlyProjection.toFixed(2)}`)

  // Compare with ECS baseline
  const ecsBaselineMonthlyCost = 950 // From baseline
  const savings = ecsBaselineMonthlyCost - monthlyProjection
  const savingsPercentage = (savings / ecsBaselineMonthlyCost) * 100

  console.log(`\n=== Cost Comparison ===\n`)
  console.log(`ECS Baseline: $${ecsBaselineMonthlyCost.toFixed(2)}/month`)
  console.log(`SST Projection: $${monthlyProjection.toFixed(2)}/month`)
  console.log(`Savings: $${savings.toFixed(2)}/month (${savingsPercentage.toFixed(2)}%)`)

  if (monthlyProjection <= ecsBaselineMonthlyCost) {
    console.log(`\n‚úÖ Cost target met: SST ‚â§ ECS`)
  } else {
    console.log(`\n‚ùå Cost target NOT met: SST > ECS`)
    process.exit(1)
  }
}

main()
```

### Cold Start Analysis

```typescript
// scripts/analyze-cold-starts.ts
import { CloudWatchClient, GetMetricStatisticsCommand } from '@aws-sdk/client-cloudwatch'

const cloudwatch = new CloudWatchClient({ region: process.env.AWS_REGION })

async function analyzeColdStarts(functionName: string) {
  const endTime = new Date()
  const startTime = new Date(endTime.getTime() - 24 * 60 * 60 * 1000) // Last 24 hours

  // Get Init Duration metric (cold starts)
  const response = await cloudwatch.send(new GetMetricStatisticsCommand({
    Namespace: 'AWS/Lambda',
    MetricName: 'Duration',
    Dimensions: [
      {
        Name: 'FunctionName',
        Value: functionName,
      },
    ],
    StartTime: startTime,
    EndTime: endTime,
    Period: 300, // 5 minutes
    Statistics: ['p99'],
    ExtendedStatistics: ['p99'],
  }))

  const datapoints = response.Datapoints || []
  const p99Values = datapoints.map((dp) => dp.ExtendedStatistics?.['p99'] || 0)
  const maxP99 = Math.max(...p99Values)

  return {
    functionName,
    coldStartP99: maxP99,
    sampleSize: datapoints.length,
  }
}

async function main() {
  const functions = [
    'lego-api-MocFunction',
    'lego-api-GalleryFunction',
    'lego-api-WishlistFunction',
    'lego-api-ProfileFunction',
  ]

  console.log('\n=== Cold Start Analysis ===\n')

  for (const func of functions) {
    const result = await analyzeColdStarts(func)
    const status = result.coldStartP99 < 2000 ? '‚úÖ' : '‚ùå'
    console.log(`${status} ${result.functionName}: ${result.coldStartP99.toFixed(2)} ms (p99)`)
  }
}

main()
```

### Validation Report Template

```markdown
# LEGO API SST Migration - Performance & Cost Validation Report

**Date**: 2025-01-15
**Performed By**: DevOps Team
**Environment**: Production SST

---

## Executive Summary

‚úÖ **VALIDATION PASSED**: All performance and cost targets met.

The SST migration delivers:
- **30% cost reduction** compared to ECS baseline
- **Response time p95: 380ms** (24% faster than ECS)
- **Throughput: 150 req/sec** (50% higher than ECS)
- **Cold starts p99: 1.8s** (within 2s target)

---

## Performance Metrics

| Metric | ECS Baseline | SST Production | Target | Status |
|--------|--------------|----------------|--------|--------|
| Response Time (p95) | 450ms | 380ms | <500ms | ‚úÖ PASS |
| Response Time (p99) | 800ms | 650ms | <2000ms | ‚úÖ PASS |
| Throughput | 166 req/sec | 150 req/sec | >100 req/sec | ‚úÖ PASS |
| Error Rate | 0.5% | 0.3% | <1% | ‚úÖ PASS |
| Cache Hit Rate | 85% | 88% | >80% | ‚úÖ PASS |
| S3 Latency (p95) | 180ms | 150ms | <200ms | ‚úÖ PASS |
| OpenSearch Latency (p95) | 280ms | 250ms | <300ms | ‚úÖ PASS |
| Cold Start (p99) | N/A | 1800ms | <2000ms | ‚úÖ PASS |

---

## Cost Analysis

| Service | ECS Baseline (Monthly) | SST Production (Monthly) | Change |
|---------|------------------------|--------------------------|--------|
| Compute | $450 (ECS Fargate) | $180 (Lambda) | -60% |
| Database | $200 (RDS) | $200 (RDS) | 0% |
| Cache | $100 (ElastiCache) | $100 (ElastiCache) | 0% |
| Search | $150 (Elasticsearch) | $150 (OpenSearch) | 0% |
| Storage | $50 (S3) | $50 (S3) | 0% |
| API Gateway | N/A (ALB) | $20 (API Gateway) | N/A |
| **Total** | **$950** | **$700** | **-26%** |

‚úÖ **Cost Target Met**: SST is **$250/month** cheaper than ECS.

---

## Load Testing Results

**Test Configuration**:
- Concurrent Users: 1000
- Duration: 10 minutes
- Total Requests: 90,000

**Results**:
- Successful Requests: 89,850 (99.8%)
- Failed Requests: 150 (0.2%)
- Average Response Time: 280ms
- p95 Response Time: 380ms
- p99 Response Time: 650ms

---

## Database Connection Pool Validation

**Configuration**: Max 20 connections
**Peak Usage**: 18 connections (90% utilization)
**Average Usage**: 12 connections (60% utilization)

‚úÖ No connection exhaustion observed during load testing.

---

## Recommendations

1. **Proceed with ECS Decommission**: All performance and cost targets met.
2. **Monitor Cold Starts**: Consider provisioned concurrency if p99 exceeds 2s consistently.
3. **Database Scaling**: Current connection pool sufficient, but monitor as traffic grows.

---

## Approval

- [ ] DevOps Lead
- [ ] Engineering Manager
- [ ] Product Owner

**Approved on**: ___________
```

---

## Design Decisions

### Artillery for Load Testing

**Decision**: Use Artillery for load testing

**Rationale**:
- Open-source, free
- YAML-based configuration (easy to version control)
- Supports complex scenarios with weighted distributions
- Built-in HTML report generation
- Can integrate with CI/CD pipeline

**Alternative Considered**: k6 (rejected due to Go scripting complexity)

### 7-Day Cost Projection

**Decision**: Use 7-day actual cost to project 30-day monthly cost

**Rationale**:
- 7 days captures full week of traffic patterns (weekday + weekend)
- More accurate than 1-day projection
- Accounts for traffic variability
- AWS Cost Explorer provides granular data

### 2-Second Cold Start Threshold

**Decision**: Set p99 cold start threshold at 2 seconds

**Rationale**:
- Acceptable for infrequent operations (first invocation)
- Lambda cold starts typically 500ms-1500ms for Node.js 20.x
- 2s provides buffer for large dependencies (Sharp, AWS SDK)
- Can optimize with provisioned concurrency if needed

**Alternative Considered**: 1-second threshold (rejected as too strict)

---

## Error Scenarios

| Scenario | Resolution |
|----------|------------|
| Response time p95 >500ms | Investigate slow queries, optimize Lambda memory |
| Throughput <100 req/sec | Check Lambda concurrency limits, API Gateway throttling |
| Cost >ECS baseline | Analyze Lambda invocation count, optimize cold starts |
| Cold starts p99 >2s | Enable provisioned concurrency for frequently-used functions |
| Database connection exhaustion | Increase max connections, optimize query patterns |

---

## Performance Optimization Checklist

- [ ] Lambda memory tuned for optimal performance/cost ratio
- [ ] Database queries use proper indexes
- [ ] Redis cache hit rate >80%
- [ ] S3 objects use appropriate storage class
- [ ] OpenSearch indices optimized for search queries
- [ ] Lambda cold starts minimized (bundle size <10 MB)
- [ ] API Gateway caching enabled for frequently-accessed endpoints

---

## Dependencies

- **Artillery**: For load testing
- **AWS Cost Explorer**: For cost analysis
- **AWS CloudWatch**: For performance metrics
- **Node.js Scripts**: For metrics extraction and validation

---

## Future Enhancements

1. **Continuous Performance Testing**: Run load tests nightly in staging
2. **Cost Anomaly Detection**: Alert if daily cost >10% above baseline
3. **A/B Testing**: Compare ECS and SST side-by-side before cutover
4. **Performance Regression Tests**: Add to CI/CD pipeline
5. **Auto-Scaling Validation**: Test Lambda concurrency scaling under spike traffic

---

## Related Stories

- **Story 5.1**: CloudWatch Dashboards - Use dashboards to monitor performance during validation
- **Story 5.2**: CloudWatch Alarms - Use alarms to detect performance regressions
- **Story 5.7**: Cost Monitoring - Set up ongoing cost monitoring post-validation
- **Story 5.8**: ECS Decommission - Proceed with decommission after validation passes
